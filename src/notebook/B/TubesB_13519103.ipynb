{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHqirTMkZlB9"
      },
      "source": [
        "# Tugas Besar IF3270 - Pembelajaran Mesin Bag. B\n",
        "\n",
        "Anggota Kelompok:\n",
        "\n",
        "1. 13519103 - Bryan Rinaldo\n",
        "2. 13519135 - Naufal Alexander Suryasumirat\n",
        "3. 13519141 - Naufal Yahya Kurnianto\n",
        "4. 13519153 - Maximillian Lukman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDWeaLG-ZafX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVXnXhTzjl5E"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuiBSjD0jIc8"
      },
      "outputs": [],
      "source": [
        "# Activation functions\n",
        "## Linear\n",
        "linear = lambda x: x\n",
        "linear = np.vectorize(linear)\n",
        "## Sigmoid\n",
        "sigmoid = lambda x: 1 / (1 + math.exp(-x))\n",
        "sigmoid = np.vectorize(sigmoid)\n",
        "## ReLU\n",
        "relu = lambda x: float(max(0, x))\n",
        "relu = np.vectorize(relu)\n",
        "## Softmax\n",
        "softmax = lambda x: np.exp(x) / np.exp(x).sum(axis=0) # already used for vectors\n",
        "## Dict\n",
        "activation_functions = {\n",
        "    'linear': linear,\n",
        "    'sigmoid': sigmoid,\n",
        "    'relu': relu,\n",
        "    'softmax': softmax\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-drdpwjsX4"
      },
      "source": [
        "## Loss/Cost Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvN5hZ-aiy3o"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "## Linear, Sigmoid, ReLU\n",
        "def general_loss(predict: np.array or int, target: list or int):\n",
        "    if (isinstance(predict, type(int))): return 0.5 * ((target - predict) ** 2)\n",
        "    sum = 0\n",
        "    for i in range(len(target)):\n",
        "        sum += ((target[i] - predict[i]) ** 2)\n",
        "    return 0.5 * sum\n",
        "## Softmax\n",
        "def softmax_loss(predict: np.array, target: int):\n",
        "    return -math.log(predict[target]) # base e\n",
        "## Dict\n",
        "loss_functions = {\n",
        "    'linear': general_loss,\n",
        "    'sigmoid': general_loss,\n",
        "    'relu': general_loss,\n",
        "    'softmax': softmax_loss\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYvuu6z_jxJv"
      },
      "source": [
        "## Back-propagation Functions / Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7t1kvCi5hQ"
      },
      "outputs": [],
      "source": [
        "# Back-propagation functions (derivatives)\n",
        "## Linear\n",
        "linear_backprop = lambda x: 1\n",
        "## Sigmoid\n",
        "# sigmoid_backprop = lambda x: sigmoid(x) * (1 - sigmoid(x)) # or x * (1 - x)?\n",
        "sigmoid_backprop = lambda x: x * (1 - x)\n",
        "## ReLU\n",
        "relu_backprop = lambda x: float(x >= 0)\n",
        "relu_backprop = np.vectorize(relu_backprop)\n",
        "## Softmax\n",
        "def softmax_backprop(arr, targ):\n",
        "    arr_copy = np.copy(arr)\n",
        "    arr_copy[targ] = -(1 - arr_copy[targ])\n",
        "    return arr_copy\n",
        "## Dict\n",
        "backprop_functions = {\n",
        "    'linear': linear_backprop,\n",
        "    'sigmoid': sigmoid_backprop,\n",
        "    'relu': relu_backprop,\n",
        "    'softmax': softmax_backprop\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbpFDh600eIP"
      },
      "source": [
        "## Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNEXM6kWi_8-"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    # n_neuron: number of neuron, weights: weight matrix, activation: activation function\n",
        "    def __init__(self, n_neuron: int, weights: np.array, activation: str) -> None:\n",
        "        self.n_neuron = n_neuron # visualization purposes\n",
        "        self.weights = weights # weights (including bias)\n",
        "        self.activation = activation # activation type [linear, sigmoid, relu, softmax]\n",
        "        self.act_function = activation_functions[activation] # activation function used\n",
        "        self.loss_function = loss_functions[activation] # loss function used\n",
        "        self.backprop_functions = backprop_functions[activation] # derivative activation functions\n",
        "        self.result = [] # retain result of feed forward iteration\n",
        "        self.deltas = np.zeros_like(self.weights) # initialize delta for backprop\n",
        "        # every feed forward add result, every backprop add to delta\n",
        "\n",
        "    def calculate(self, in_matrix: np.array) -> np.array:\n",
        "        self.result = self.act_function(np.dot(self.weights.transpose(), in_matrix))\n",
        "        return self.result # [a0, a1, a2, ..., an]\n",
        "\n",
        "    def calculate_loss(self, prediction: (np.array or int), target: (list or int)) -> float: # used for calculating loss for output layer\n",
        "        if (self.activation == \"softmax\"): return self.loss_function(prediction, np.argmax(target))\n",
        "        return self.loss_function(prediction, target)\n",
        "    \n",
        "    def update_weight(self):\n",
        "        self.weights += self.deltas # adding deltas to weights\n",
        "        self.deltas = np.zeros_like(self.deltas) # resettings deltas for next mini-batch\n",
        "        return self.weights # for verbose purpose\n",
        "    \n",
        "    def add_deltas(self, delta_matrix: np.array) -> None:\n",
        "        self.deltas += delta_matrix\n",
        "        return self.deltas # for verbose purpose\n",
        "    \n",
        "    def get_structure(self) -> tuple((int, np.array, np.array)):\n",
        "        # n_neuron: int, weight matrix: np.array, bias weight matrix: np.array\n",
        "        n_neuron = self.n_neuron\n",
        "        weight_neuron = self.weights[:-1,]\n",
        "        weight_bias = self.weights[-1:,].flatten()\n",
        "        return (n_neuron, weight_neuron, weight_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZK7aIz0pCU"
      },
      "source": [
        "## FFNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5j2PgKYjE_e"
      },
      "outputs": [],
      "source": [
        "class FFNN:\n",
        "    def __init__(self,  \n",
        "            hidden_layers: list,\n",
        "            input_layer = None,\n",
        "            threshold = 0.5,\n",
        "            learning_rate = 0.01,\n",
        "            err_threshold = 0.001,\n",
        "            max_iter = 5000,\n",
        "            batch_size = 1) -> None:\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.output_layer = hidden_layers[-1]\n",
        "        self.output_activation = self.output_layer.activation\n",
        "        self.input_layer = input_layer\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "        self.err_threshold = err_threshold\n",
        "        self.max_iter = max_iter\n",
        "        self.batch_size = batch_size # default incremental SGD\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_model(input_size: int, n_neurons: list, activations: list):\n",
        "        if (len(n_neurons) != len(activations)): return None\n",
        "        arr = []\n",
        "        for i in range(len(n_neurons)):\n",
        "            if (i == 0): arr.append(Layer(\n",
        "                n_neurons[i], np.random.uniform(low = -1.0, high = 1.0, \n",
        "                    size = (input_size + 1, n_neurons[i])),\n",
        "                    activations[i])\n",
        "                )\n",
        "            else: arr.append(Layer(\n",
        "                n_neurons[i], np.random.uniform(low = -1.0, high = 1.0,\n",
        "                    size = (n_neurons[i - 1] + 1, n_neurons[i])),\n",
        "                    activations[i])\n",
        "                )\n",
        "        return FFNN(arr)\n",
        "\n",
        "    def feed_forward(self) -> (np.array or None):\n",
        "        if (isinstance(self.input_layer, type(None))): return None\n",
        "        if len(self.input_layer.shape) == 1: return self.forward(self.input_layer)\n",
        "        else:\n",
        "            outputs = []\n",
        "            for data in self.input_layer: outputs.append(self.forward(data))\n",
        "            if (self.output_layer.activation == 'softmax'): return outputs\n",
        "            return np.array(outputs).flatten()\n",
        "    \n",
        "    def forward(self, input) -> (np.array or None):\n",
        "        output = input\n",
        "        for i in range(0, len(self.hidden_layers)):\n",
        "            output = self.hidden_layers[i].calculate(np.append(output, 1))\n",
        "        if (self.output_layer.activation == 'softmax'): return output # usually used for multiclass\n",
        "        if (self.output_layer.n_neuron > 1): return np.where(output > self.threshold, 1, 0) # multiclass non-softmax\n",
        "        return int(output > self.threshold) # binary\n",
        "    \n",
        "    def fit(self, x_train, y_train, randomize = False, learning_rate = None, \n",
        "            batch_size = None, max_iter = None, \n",
        "            err_threshold = None, update_every = 250) -> None:\n",
        "        if learning_rate is not None: self.learning_rate = learning_rate\n",
        "        if batch_size is not None: self.batch_size = batch_size\n",
        "        if max_iter is not None: self.max_iter = max_iter\n",
        "        if err_threshold is not None: self.err_threshold = err_threshold\n",
        "        for epoch in range(self.max_iter):\n",
        "            training_data = x_train\n",
        "            training_target = y_train\n",
        "            if randomize:\n",
        "                pass # randomize dataset x_train here\n",
        "            error_sum = 0 # initialize error (for comparing with err_threshold)\n",
        "            for iter in range(len(y_train)):\n",
        "                pred = self.predict(training_data[iter]) # results already encoded\n",
        "                pred = self.output_layer.result # result before encoded\n",
        "                error = self.output_layer.calculate_loss(pred, training_target[iter])\n",
        "                self.backpropagate(training_data[iter], training_target[iter])\n",
        "                error_sum += error\n",
        "                if ((iter + 1) % self.batch_size == 0 or iter == len(training_target) - 1):\n",
        "                    self.update_weights() # update weights (mini-batch)\n",
        "            err_avg = error_sum / len(y_train)\n",
        "\n",
        "            if (err_avg < self.err_threshold):\n",
        "                print(\"Epoch %d, Loss: %.6f | Reason for stopping: err < err_threshold\" % (epoch, err_avg))\n",
        "                break # stop fitting process when avg error < threshold\n",
        "\n",
        "            if (epoch % update_every == 0):\n",
        "                print(\"Epoch %d, Loss: %.6f\" % (1 if epoch == 0 else epoch, err_avg))\n",
        "        return\n",
        "    \n",
        "    def backpropagate(self, input, target): # update deltas for every layer\n",
        "        err_term = 0\n",
        "        for iter in reversed(range(0, len(self.hidden_layers))):\n",
        "            prev_layer = None if iter == 0 else self.hidden_layers[iter - 1]\n",
        "            prev_result = np.atleast_2d(np.append(input, 1)) if prev_layer == None \\\n",
        "                else np.atleast_2d(np.append(prev_layer.result, 1))\n",
        "            if (iter == len(self.hidden_layers) - 1): # if output layer\n",
        "                if (self.output_activation == \"softmax\"): # if softmax output layer\n",
        "                    pred = self.output_layer.result\n",
        "                    err_deriv = self.output_layer.backprop_functions(pred, np.argmax(target))\n",
        "                    err_term = err_deriv\n",
        "                    gradient = np.dot(prev_result.T,\n",
        "                        np.atleast_2d(err_deriv))\n",
        "                    delta = -self.learning_rate * gradient\n",
        "                    self.output_layer.add_deltas(delta)\n",
        "                    pass\n",
        "                else: # if other output layer\n",
        "                    pred = self.output_layer.result\n",
        "                    err_deriv = -(np.array(target) - pred)\n",
        "                    err_term = err_deriv\n",
        "                    donet = self.output_layer.backprop_functions(pred)\n",
        "                    gradient = np.dot(prev_result.T,\n",
        "                        np.atleast_2d(err_deriv * donet))\n",
        "                    delta = -self.learning_rate * gradient\n",
        "                    self.output_layer.add_deltas(delta)\n",
        "            else: # if hidden layer\n",
        "                this_layer = self.hidden_layers[iter]\n",
        "                next_layer = self.hidden_layers[iter + 1]\n",
        "                err_term = np.add.reduce(next_layer.weights[:-1].T * \n",
        "                    np.atleast_2d(err_term).T, 0) / np.shape(err_term)[0]\n",
        "                donet = this_layer.backprop_functions(this_layer.result) # no softmax in hidden layer\n",
        "                gradient = np.dot(prev_result.T,\n",
        "                    np.atleast_2d(err_term * donet))\n",
        "                delta = -self.learning_rate * gradient\n",
        "                self.hidden_layers[iter].add_deltas(delta)\n",
        "                pass\n",
        "        return\n",
        "        \n",
        "    def update_weights(self):\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_weight()\n",
        "        return\n",
        "    \n",
        "    def attach_input(self, input_layer: np.array) -> None:\n",
        "        self.input_layer = input_layer\n",
        "        return\n",
        "\n",
        "    def attach_hidden_layer(self, hidden_layer: Layer) -> None:\n",
        "        self.hidden_layers.append(hidden_layer)\n",
        "        return\n",
        "\n",
        "    def predict(self, input_layer: np.array) -> list: # input_layer without bias\n",
        "        self.input_layer = input_layer\n",
        "        return self.feed_forward()\n",
        "\n",
        "    def get_structure(self) -> tuple((np.array, list)):\n",
        "        return (self.input_layer, [layer.get_structure() for layer in self.hidden_layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3J3Ew10rPI"
      },
      "source": [
        "## Accuracy Calculation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ1rchygjU8g"
      },
      "outputs": [],
      "source": [
        "# Fungsi menghitung akurasi dari model\n",
        "def calculate_accuracy(model: FFNN, input_set, validation_set: list, is_softmax = False):\n",
        "    # returns range from 1..100 (percentage)\n",
        "    predicted_set = model.predict(np.array(input_set))\n",
        "    if (not isinstance(predicted_set, (list, np.ndarray))): return int(predicted_set == validation_set[0]) * 100\n",
        "    if (len(predicted_set) != len(validation_set)): return None\n",
        "    num_correct = 0\n",
        "    for i in range(len(predicted_set)):\n",
        "        if is_softmax:\n",
        "            if (np.argmax(predicted_set[i]) == np.argmax(validation_set[i])): num_correct += 1\n",
        "        else:\n",
        "            if predicted_set[i].tolist() == validation_set[i].tolist(): num_correct += 1\n",
        "    return num_correct / len(validation_set) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgJ5O7DV0vbd"
      },
      "source": [
        "## Input Model File Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytp347DRjFwn"
      },
      "outputs": [],
      "source": [
        "# Fungsi membaca input file\n",
        "def _input(filename: str, with_input = False) -> tuple((FFNN, np.array, np.array)):\n",
        "    f = open(filename, \"r\")\n",
        "    f = f.readlines()\n",
        "    f = [line.strip() for line in f]\n",
        "\n",
        "    nLayer = int(f[0])\n",
        "    f = f[1:]\n",
        "    n_layer_neurons = []\n",
        "    struct_model = {}\n",
        "\n",
        "    for i in range(nLayer-1):\n",
        "        struct_model[i] = {}\n",
        "        n_layer_neurons.append(int(f[0]))\n",
        "        struct_model[i][\"b\"] = [float(b) for b in f[1].split()]\n",
        "        struct_model[i][\"w\"] = [[float(w) for w in weights.split()] for weights in f[2:(2 + int(f[0]))]]\n",
        "        struct_model[i][\"f\"] = f[2 + int(f[0])]\n",
        "        f = f[2 + int(f[0]) + 1:]\n",
        "\n",
        "    n_layer_neurons.append(int(f[0]))\n",
        "    \n",
        "    if (with_input):\n",
        "        n_input = int(f[1])\n",
        "        f = f[2:]\n",
        "        input_data = []\n",
        "        for i in range(n_input):\n",
        "            input = [int(x) for x in (f[i].split())]\n",
        "            input_data.append(input)\n",
        "\n",
        "        f = f[n_input:]\n",
        "        validation_data = []\n",
        "        for i in range(n_input):\n",
        "            result = [int(y) for y in (f[i].split())]\n",
        "            validation_data.append(result)\n",
        "\n",
        "    model_layers = []\n",
        "    for i in range (nLayer-1):\n",
        "        weight = struct_model[i][\"w\"]\n",
        "        weight.append(struct_model[i][\"b\"])\n",
        "        layer = Layer(n_layer_neurons[i+1], np.array(weight), struct_model[i][\"f\"].lower())\n",
        "        model_layers.append(layer)\n",
        "    \n",
        "    if (with_input):\n",
        "        return FFNN(model_layers, np.array(input_data)), input_data, validation_data\n",
        "    else:\n",
        "        return FFNN(model_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ww-1Adh0yEA"
      },
      "source": [
        "## Show Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz8N1pNYjPf0"
      },
      "outputs": [],
      "source": [
        "# Memperlihatkan koefisien dan struktur dari model\n",
        "def showModel(model: FFNN): #masukan berupa FFNN\n",
        "    initLayers = model.get_structure()\n",
        "    countLayer = len(initLayers[1])\n",
        "    print(\"==============Model FFNN==============\\n\")\n",
        "    print(\"------------------------------\")\n",
        "    for j in range(0, countLayer):\n",
        "        weight = initLayers[1][j][1]\n",
        "        bias = initLayers[1][j][2]\n",
        "\n",
        "        if (j == (countLayer - 1)):\n",
        "            print(\"------ Output Layer ------\" )\n",
        "            print(\"Weight: \" , weight)\n",
        "            print(\"Bias: \" , bias)\n",
        "            print('\\n')\n",
        "            print(\"------------------------------\")\n",
        "        else:\n",
        "            print(\"--- Hidden Layer %d ---\" %(j+1))\n",
        "            print(\"H%d Weight: \" %(j+1), weight)\n",
        "            print(\"H%d Bias: \" %(j+1), bias)\n",
        "            print('\\n')\n",
        "            print(\"------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYPhpLIo4w2"
      },
      "source": [
        "## Fungsi Formatting Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ef3O3I-3Aq-"
      },
      "outputs": [],
      "source": [
        "def input_dataset(dataset_input):\n",
        "  x_iris = dataset_input.data\n",
        "  y_iris = dataset_input.target\n",
        "\n",
        "  y_iris_temp = [] \n",
        "  maxElement = np.amax(y_iris)\n",
        "  for x in range(len(y_iris)):\n",
        "    iris = []\n",
        "    i = 0\n",
        "\n",
        "    # CEK UDH FULL\n",
        "    while len(iris) < (maxElement+1) :\n",
        "      if(i == y_iris[x]):\n",
        "        iris.append(1)\n",
        "      else:\n",
        "        iris.append(0)\n",
        "      i+=1\n",
        "    y_iris_temp.append(iris)\n",
        "\n",
        "  y_iris_return = np.array(y_iris_temp)\n",
        "  returnDict = {\n",
        "      \"x_train\" : x_iris,\n",
        "      \"y_train\" : y_iris_return\n",
        "  }\n",
        "  return returnDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ea4KYXz59D"
      },
      "source": [
        "## Fit Kelas FFNN (backprop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyJ0Li7R0FHE"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aSprbANjdg8",
        "outputId": "512a7231-8b38-4aa5-aadc-8450258ad6aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Iris\n",
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                  5.1               3.5                1.4               0.2\n",
            "1                  4.9               3.0                1.4               0.2\n",
            "2                  4.7               3.2                1.3               0.2\n",
            "3                  4.6               3.1                1.5               0.2\n",
            "4                  5.0               3.6                1.4               0.2\n",
            "..                 ...               ...                ...               ...\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n",
            "\n",
            "[150 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "dataIris = datasets.load_iris()\n",
        "df = pd.DataFrame(data=dataIris.data, columns=dataIris.feature_names)\n",
        "\n",
        "print(\"Data Iris\")\n",
        "print(df)\n",
        "\n",
        "dataset = input_dataset(dataIris) # using input_dataset function\n",
        "\n",
        "x_train = dataset['x_train']\n",
        "y_train = dataset['y_train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d6BRF5O4eiK"
      },
      "source": [
        "## Defining FFNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fungsi FFNN.generate_model\n",
        "\n",
        "Digunakan untuk mendefinisikan model awal yang akan digunakan untuk training (random weight diantara -1.0 dan 1.0)"
      ],
      "metadata": {
        "id": "SOs6E3RyBJfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "input_size: size atau panjang dari input data / x_train\n",
        "\n",
        "n_neurons: banyak neuron untuk tiap layer\n",
        "\n",
        "activations: fungsi aktivasi untuk setiap layer"
      ],
      "metadata": {
        "id": "P3dTBwMDA4ZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eLL_s_Q0Jw2"
      },
      "outputs": [],
      "source": [
        "ffnn_model = FFNN.generate_model(\n",
        "    4, [6, 4, 5, 3], ['sigmoid', 'relu', 'linear', 'softmax']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krZBFEJ24wwS"
      },
      "source": [
        "## Model Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-OtUuKH4oaq",
        "outputId": "2d04a4e4-9e6d-4b1e-c562-e2f5074ba7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Model FFNN==============\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 1 ---\n",
            "H1 Weight:  [[ 0.66369284 -0.45270365 -0.42983886 -0.85583888  0.73228471 -0.80021572]\n",
            " [-0.79724836  0.31760963 -0.86537262 -0.3157094  -0.99836713  0.61790338]\n",
            " [ 0.25208063 -0.03818579  0.65917266  0.68604262 -0.87913485 -0.4246658 ]\n",
            " [-0.69237659  0.80651322 -0.09485453  0.55354486 -0.20639417  0.74513472]]\n",
            "H1 Bias:  [-0.45344607 -0.55241345 -0.25748296  0.87481449  0.2647077  -0.60764919]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 2 ---\n",
            "H2 Weight:  [[-0.6878233  -0.76034618 -0.19083851  0.34942044]\n",
            " [-0.44804084 -0.07654788  0.1594647  -0.89866458]\n",
            " [ 0.26774823 -0.13340287 -0.62994191 -0.35570497]\n",
            " [ 0.93109576 -0.02493166 -0.91936429 -0.8340818 ]\n",
            " [-0.06191021 -0.01641168  0.36525006 -0.98274208]\n",
            " [ 0.24841938 -0.71940122 -0.49256429 -0.39798024]]\n",
            "H2 Bias:  [-0.41645079  0.05962058  0.72659033 -0.67594457]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 3 ---\n",
            "H3 Weight:  [[ 0.83439762  0.80329118 -0.9011956  -0.18916249 -0.67409527]\n",
            " [-0.93132128 -0.38951473 -0.08718598 -0.95811298  0.07170306]\n",
            " [-0.74771459 -0.45757586  0.80989936 -0.4737613   0.69840811]\n",
            " [ 0.8994165   0.98603479 -0.1646545  -0.88898268 -0.58902157]]\n",
            "H3 Bias:  [-0.92826209 -0.48962686  0.89083764  0.10170513 -0.55472709]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "------ Output Layer ------\n",
            "Weight:  [[-0.14456545 -0.16371309  0.24329187]\n",
            " [-0.62825125  0.93905573  0.48150628]\n",
            " [ 0.92672271 -0.27641882  0.80113287]\n",
            " [-0.24638949  0.43207012 -0.57199489]\n",
            " [-0.93268059 -0.86319888  0.9572248 ]]\n",
            "Bias:  [-0.64655918  0.55477061  0.51398339]\n",
            "\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "showModel(ffnn_model) # showing model before training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZ1_5-S4ydU"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "x_train: data untuk training\n",
        "\n",
        "y_train: target untuk training\n",
        "\n",
        "learning_rate: learning rate untuk dikalikan dengan gradien\n",
        "\n",
        "batch_size: size mini-batch\n",
        "\n",
        "err_threshold: threshold error, kasus berhenti jika error < error threshold\n",
        "\n",
        "max_iter: iterasi maksimal (epoch)\n",
        "\n",
        "update_every: memberikan update setiap x iterasi"
      ],
      "metadata": {
        "id": "radOL5dpAajc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp-yTIIB40MR",
        "outputId": "b1bca0da-eb76-40f4-f47a-a3466172c622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.978117\n",
            "Epoch 250, Loss: 0.058231\n",
            "Epoch 500, Loss: 0.053985\n",
            "Epoch 750, Loss: 0.051862\n"
          ]
        }
      ],
      "source": [
        "ffnn_model.fit(x_train, y_train, learning_rate = 0.01, batch_size = 4, err_threshold = 0.01, max_iter = 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tgMJrSx5Cqf"
      },
      "source": [
        "## Model After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKH5foKY44_b",
        "outputId": "71d95644-d5ad-4305-c2d8-6b25b09f3a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Model FFNN==============\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 1 ---\n",
            "H1 Weight:  [[ 0.65116446 -0.57562676 -0.01249252 -0.55501137  0.87979008 -0.87055799]\n",
            " [-1.03946763  0.3993611  -1.04980528 -0.66744711 -0.71195099  0.56682225]\n",
            " [ 0.33312991 -0.17847254  0.83043653  0.79964899 -0.96828172 -0.44995993]\n",
            " [-0.44689255  0.61963042  0.46531022  1.32859817 -0.64693884  0.74321905]]\n",
            "H1 Bias:  [-0.54651916 -0.51823667 -0.38754463  0.62481382  0.43956705 -0.62300903]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 2 ---\n",
            "H2 Weight:  [[-0.16688791 -1.09655799 -0.97738668  0.59159437]\n",
            " [-0.61223257 -0.05802843  0.25959456 -0.95884851]\n",
            " [ 0.98794726 -0.46748047 -1.54225814 -0.0189208 ]\n",
            " [ 1.7279013  -0.50425013 -2.04179827 -0.40535579]\n",
            " [-0.94482137  0.30516561  1.1579046  -1.45113032]\n",
            " [ 0.15671981 -0.68392457 -0.40545831 -0.44205489]]\n",
            "H2 Bias:  [-1.0419827   0.06284684  0.87350288 -0.96249756]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 3 ---\n",
            "H3 Weight:  [[ 2.21253589  1.92109744 -1.72545139 -0.34228526 -0.52154501]\n",
            " [-0.93132128 -0.38951473 -0.08718598 -0.95811298  0.07170306]\n",
            " [-1.02897392 -1.99744546  1.62134798 -1.44798207  0.83941058]\n",
            " [ 0.8994165   0.98603479 -0.1646545  -0.88898268 -0.58902157]]\n",
            "H3 Bias:  [-1.77204555 -0.46382896  1.00020948  0.88293711 -0.57717185]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "------ Output Layer ------\n",
            "Weight:  [[-1.2859421  -2.11855167  3.3395071 ]\n",
            " [-2.99369303  0.3396424   3.44636139]\n",
            " [ 2.75090664  0.97336265 -2.27283253]\n",
            " [-2.18689042  1.89101923 -0.09044307]\n",
            " [ 1.09588657 -1.2515198  -0.68302144]]\n",
            "Bias:  [-1.82482467  1.93909507  0.30792441]\n",
            "\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "showModel(ffnn_model) # showing model after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AziE7a3C5KrW"
      },
      "source": [
        "## Predictions After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMRa97Mv5If3",
        "outputId": "7a80b022-bb3a-4c59-9c6a-5f6171a5c9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.99997117e-01 2.88311033e-06 3.47068008e-21]\n",
            "[9.99965608e-01 3.43917006e-05 3.23788012e-19]\n",
            "[9.99991374e-01 8.62559964e-06 2.57762305e-20]\n",
            "[9.99911143e-01 8.88568519e-05 1.83885941e-18]\n",
            "[9.99997521e-01 2.47857952e-06 2.63196213e-21]\n",
            "[9.99991830e-01 8.17025516e-06 2.33412020e-20]\n",
            "[9.99982350e-01 1.76503155e-05 9.55410852e-20]\n",
            "[9.99989576e-01 1.04238386e-05 3.64494636e-20]\n",
            "[9.99839693e-01 1.60307123e-04 5.41321380e-18]\n",
            "[9.99968414e-01 3.15860600e-05 2.77100660e-19]\n",
            "[9.99998123e-01 1.87688537e-06 1.58239974e-21]\n",
            "[9.99964011e-01 3.59894934e-05 3.51841220e-19]\n",
            "[9.99972791e-01 2.72092007e-05 2.10915742e-19]\n",
            "[9.99993899e-01 6.10131500e-06 1.36802798e-20]\n",
            "[9.99999913e-01 8.72493960e-08 5.76669900e-24]\n",
            "[9.99999623e-01 3.76873431e-07 8.38640088e-23]\n",
            "[9.99999524e-01 4.76378901e-07 1.28753736e-22]\n",
            "[9.99996025e-01 3.97492623e-06 6.24594878e-21]\n",
            "[9.99995420e-01 4.58001918e-06 8.09458832e-21]\n",
            "[9.99996663e-01 3.33731535e-06 4.53592814e-21]\n",
            "[9.99974886e-01 2.51137799e-05 1.82149553e-19]\n",
            "[9.99993649e-01 6.35064547e-06 1.47204653e-20]\n",
            "[9.99999643e-01 3.57161688e-07 7.60129606e-23]\n",
            "[9.99726648e-01 2.73351504e-04 1.43734713e-17]\n",
            "[9.99479069e-01 5.20931306e-04 4.67814002e-17]\n",
            "[9.99845999e-01 1.54001370e-04 5.02996039e-18]\n",
            "[9.99945775e-01 5.42251948e-05 7.44878271e-19]\n",
            "[9.99994938e-01 5.06230532e-06 9.72191776e-21]\n",
            "[9.99996359e-01 3.64082880e-06 5.31905188e-21]\n",
            "[9.99893512e-01 1.06488454e-04 2.56088493e-18]\n",
            "[9.99864018e-01 1.35982208e-04 4.00568531e-18]\n",
            "[9.99989221e-01 1.07791694e-05 3.87549569e-20]\n",
            "[9.99999223e-01 7.77205413e-07 3.15298838e-22]\n",
            "[9.99999753e-01 2.46624275e-07 3.86028741e-23]\n",
            "[9.99953068e-01 4.69322293e-05 5.71881540e-19]\n",
            "[9.99997361e-01 2.63942261e-06 2.95285051e-21]\n",
            "[9.99999034e-01 9.65863912e-07 4.69255315e-22]\n",
            "[9.99997810e-01 2.19033170e-06 2.09912268e-21]\n",
            "[9.99960385e-01 3.96149218e-05 4.19387319e-19]\n",
            "[9.99991262e-01 8.73827570e-06 2.63956536e-20]\n",
            "[9.99997704e-01 2.29567387e-06 2.28751618e-21]\n",
            "[9.95355949e-01 4.64405122e-03 2.57030976e-15]\n",
            "[9.99983029e-01 1.69708226e-05 8.89189674e-20]\n",
            "[9.99901561e-01 9.84387032e-05 2.21782644e-18]\n",
            "[9.99874031e-01 1.25969264e-04 3.48253399e-18]\n",
            "[9.99936988e-01 6.30120693e-05 9.80451065e-19]\n",
            "[9.99995036e-01 4.96396708e-06 9.37915976e-21]\n",
            "[9.99976008e-01 2.39919235e-05 1.67538158e-19]\n",
            "[9.99997820e-01 2.18041810e-06 2.08177177e-21]\n",
            "[9.99992735e-01 7.26549359e-06 1.88305269e-20]\n",
            "[2.50590876e-04 9.99748026e-01 1.38315710e-06]\n",
            "[7.24303705e-05 9.99852074e-01 7.54959273e-05]\n",
            "[3.15018380e-05 9.98869021e-01 1.09947684e-03]\n",
            "[2.00562942e-05 9.95340085e-01 4.63985829e-03]\n",
            "[2.07042980e-05 9.95783402e-01 4.19589357e-03]\n",
            "[1.18282411e-05 9.76522404e-01 2.34657679e-02]\n",
            "[1.83225535e-05 9.93812828e-01 6.16884976e-03]\n",
            "[8.20507907e-04 9.99179462e-01 3.02038821e-08]\n",
            "[1.01605727e-04 9.99873024e-01 2.53699605e-05]\n",
            "[2.43341531e-05 9.97464543e-01 2.51112305e-03]\n",
            "[1.35402293e-04 9.99854541e-01 1.00568845e-05]\n",
            "[4.67638784e-05 9.99644366e-01 3.08870482e-04]\n",
            "[3.60220973e-04 9.99639350e-01 4.29393517e-07]\n",
            "[1.01337245e-05 9.63499007e-01 3.64908592e-02]\n",
            "[9.62422137e-04 9.99037560e-01 1.80536651e-08]\n",
            "[3.66966311e-04 9.99632629e-01 4.04465947e-07]\n",
            "[5.76709333e-06 8.60665414e-01 1.39328819e-01]\n",
            "[4.03481970e-04 9.99596220e-01 2.97900248e-07]\n",
            "[3.30665329e-06 6.83644033e-01 3.16352661e-01]\n",
            "[2.04678827e-04 9.99792666e-01 2.65553614e-06]\n",
            "[2.12410307e-07 1.07564368e-01 8.92435420e-01]\n",
            "[3.85490062e-04 9.99614165e-01 3.45093550e-07]\n",
            "[3.16549481e-07 1.44397983e-01 8.55601700e-01]\n",
            "[2.16172793e-05 9.96318846e-01 3.65953686e-03]\n",
            "[2.59082077e-04 9.99739676e-01 1.24229700e-06]\n",
            "[2.06433364e-04 9.99790983e-01 2.58347854e-06]\n",
            "[3.12037499e-05 9.98835278e-01 1.13351842e-03]\n",
            "[2.13113116e-06 5.35487261e-01 4.64510608e-01]\n",
            "[1.17665438e-05 9.76160968e-01 2.38272651e-02]\n",
            "[1.03665116e-03 9.98963335e-01 1.42056525e-08]\n",
            "[1.96628165e-04 9.99800350e-01 3.02216836e-06]\n",
            "[6.01938618e-04 9.99397979e-01 8.20230847e-08]\n",
            "[3.59608744e-04 9.99639960e-01 4.31754567e-07]\n",
            "[2.56791454e-09 3.79810555e-03 9.96201892e-01]\n",
            "[3.00449728e-06 6.50574490e-01 3.49422506e-01]\n",
            "[3.01152290e-05 9.98699701e-01 1.27018392e-03]\n",
            "[4.69102958e-05 9.99647311e-01 3.05778760e-04]\n",
            "[2.87909625e-05 9.98504191e-01 1.46701783e-03]\n",
            "[1.01076800e-04 9.99873123e-01 2.58002282e-05]\n",
            "[3.18562638e-05 9.98907424e-01 1.06071943e-03]\n",
            "[1.10029844e-05 9.71058748e-01 2.89302487e-02]\n",
            "[2.07332059e-05 9.95801867e-01 4.17739959e-03]\n",
            "[1.62769987e-04 9.99831673e-01 5.55672911e-06]\n",
            "[8.50393280e-04 9.99149580e-01 2.69119914e-08]\n",
            "[2.71801178e-05 9.98208970e-01 1.76384965e-03]\n",
            "[1.39520632e-04 9.99851348e-01 9.13124859e-06]\n",
            "[6.04946207e-05 9.99804665e-01 1.34839880e-04]\n",
            "[1.44213789e-04 9.99847578e-01 8.20780119e-06]\n",
            "[1.03665116e-03 9.98963335e-01 1.42056525e-08]\n",
            "[7.36555470e-05 9.99854820e-01 7.15247389e-05]\n",
            "[1.12279666e-13 1.79129461e-06 9.99998209e-01]\n",
            "[3.39014184e-11 1.39863807e-04 9.99860136e-01]\n",
            "[8.80597398e-12 4.99959400e-05 9.99950004e-01]\n",
            "[2.89786419e-11 1.24081187e-04 9.99875919e-01]\n",
            "[8.78970929e-13 8.61340035e-06 9.99991387e-01]\n",
            "[4.81190883e-13 5.43864735e-06 9.99994561e-01]\n",
            "[3.75288168e-10 8.75935423e-04 9.99124064e-01]\n",
            "[9.02567431e-12 5.09450611e-05 9.99949055e-01]\n",
            "[6.31801559e-12 3.88053797e-05 9.99961195e-01]\n",
            "[1.93976741e-12 1.57589205e-05 9.99984241e-01]\n",
            "[1.24147473e-08 1.26159773e-02 9.87384010e-01]\n",
            "[9.74485539e-11 3.13066398e-04 9.99686934e-01]\n",
            "[6.95864279e-11 2.42120842e-04 9.99757879e-01]\n",
            "[8.66537935e-12 4.93856199e-05 9.99950614e-01]\n",
            "[1.37431281e-12 1.21146386e-05 9.99987885e-01]\n",
            "[3.25320765e-11 1.35531653e-04 9.99864468e-01]\n",
            "[3.77482575e-10 8.79840635e-04 9.99120159e-01]\n",
            "[4.87603634e-12 3.18439985e-05 9.99968156e-01]\n",
            "[2.42568254e-14 5.56307353e-07 9.99999444e-01]\n",
            "[1.73076429e-09 2.81130816e-03 9.97188690e-01]\n",
            "[8.41948512e-12 4.83125169e-05 9.99951687e-01]\n",
            "[6.22085497e-11 2.22273772e-04 9.99777726e-01]\n",
            "[4.39848085e-13 5.07828203e-06 9.99994922e-01]\n",
            "[1.53450409e-08 1.48225778e-02 9.85177407e-01]\n",
            "[3.95328850e-11 1.57266704e-04 9.99842733e-01]\n",
            "[4.06325986e-10 9.30684769e-04 9.99069315e-01]\n",
            "[5.50807679e-08 3.90780662e-02 9.60921879e-01]\n",
            "[3.64096897e-08 2.85660713e-02 9.71433892e-01]\n",
            "[2.17801963e-12 1.72155975e-05 9.99982784e-01]\n",
            "[1.62666613e-08 1.54947457e-02 9.84505238e-01]\n",
            "[2.08740119e-11 9.66008505e-05 9.99903399e-01]\n",
            "[1.98497761e-09 3.12102696e-03 9.96878971e-01]\n",
            "[1.08958617e-12 1.01476775e-05 9.99989852e-01]\n",
            "[1.75028849e-07 9.31455253e-02 9.06854300e-01]\n",
            "[1.31884579e-10 3.94385031e-04 9.99605615e-01]\n",
            "[4.92302248e-12 3.20779059e-05 9.99967922e-01]\n",
            "[3.15220617e-12 2.28269388e-05 9.99977173e-01]\n",
            "[4.28736132e-10 9.69598317e-04 9.99030401e-01]\n",
            "[6.86583675e-08 4.61531712e-02 9.53846760e-01]\n",
            "[5.87600868e-10 1.23320229e-03 9.98766797e-01]\n",
            "[2.45974544e-12 1.88902623e-05 9.99981110e-01]\n",
            "[1.50861086e-09 2.53167023e-03 9.97468328e-01]\n",
            "[3.39014184e-11 1.39863807e-04 9.99860136e-01]\n",
            "[1.36637702e-12 1.20612164e-05 9.99987939e-01]\n",
            "[1.51741885e-12 1.30659520e-05 9.99986934e-01]\n",
            "[9.81666271e-11 3.14825254e-04 9.99685175e-01]\n",
            "[4.44892004e-10 9.97352443e-04 9.99002647e-01]\n",
            "[8.87863865e-10 1.68963495e-03 9.98310364e-01]\n",
            "[2.33077339e-11 1.05082539e-04 9.99894917e-01]\n",
            "[1.04230027e-09 1.90950607e-03 9.98090493e-01]\n"
          ]
        }
      ],
      "source": [
        "for data in x_train:\n",
        "    print(ffnn_model.predict(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBshra805hHB"
      },
      "source": [
        "## Accuracy of Model After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt96CwzI2ra8",
        "outputId": "77ef4d95-4b9b-4052-a801-93c150c6fe62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Model: 98.00%\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of Model: %.2f\" % (calculate_accuracy(ffnn_model, x_train, y_train, is_softmax = True)) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqn-Czs-uOdj",
        "outputId": "30cf1ff2-8cab-4648-aa17-20b1fb3df990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Prediction\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "\n",
            "Probability Prediction\n",
            "[[9.99671403e-01 3.28597256e-04 8.48919426e-24]\n",
            " [9.99299698e-01 7.00301566e-04 6.53435910e-23]\n",
            " [9.99485826e-01 5.14173823e-04 2.70846705e-23]\n",
            " [9.99087652e-01 9.12348492e-04 1.51733461e-22]\n",
            " [9.99689064e-01 3.10936053e-04 7.42235442e-24]\n",
            " [9.99518586e-01 4.81414395e-04 3.89875777e-23]\n",
            " [9.99316636e-01 6.83364202e-04 7.61452759e-23]\n",
            " [9.99539206e-01 4.60794024e-04 2.28655834e-23]\n",
            " [9.98816300e-01 1.18370015e-03 2.89501726e-22]\n",
            " [9.99462025e-01 5.37975310e-04 2.95499538e-23]\n",
            " [9.99740335e-01 2.59664648e-04 4.73803205e-24]\n",
            " [9.99375332e-01 6.24667540e-04 5.76567679e-23]\n",
            " [9.99448432e-01 5.51567513e-04 2.94076084e-23]\n",
            " [9.99539126e-01 4.60874000e-04 1.51002268e-23]\n",
            " [9.99886141e-01 1.13858670e-04 4.28852674e-25]\n",
            " [9.99819579e-01 1.80420927e-04 2.32317373e-24]\n",
            " [9.99740936e-01 2.59064442e-04 5.50182190e-24]\n",
            " [9.99568515e-01 4.31485342e-04 2.06025692e-23]\n",
            " [9.99656908e-01 3.43091869e-04 1.29073774e-23]\n",
            " [9.99644523e-01 3.55476778e-04 1.32227489e-23]\n",
            " [9.99499860e-01 5.00139969e-04 3.15816220e-23]\n",
            " [9.99467767e-01 5.32233169e-04 4.60097452e-23]\n",
            " [9.99781848e-01 2.18151800e-04 2.24475619e-24]\n",
            " [9.98005715e-01 1.99428514e-03 2.41175296e-21]\n",
            " [9.98895402e-01 1.10459786e-03 3.47305683e-22]\n",
            " [9.99049563e-01 9.50437446e-04 1.73577252e-22]\n",
            " [9.98961916e-01 1.03808406e-03 3.12921905e-22]\n",
            " [9.99638211e-01 3.61788550e-04 1.16935959e-23]\n",
            " [9.99652316e-01 3.47683746e-04 9.73764326e-24]\n",
            " [9.99112790e-01 8.87209722e-04 1.50987747e-22]\n",
            " [9.99046853e-01 9.53147099e-04 1.80484965e-22]\n",
            " [9.99355668e-01 6.44332343e-04 7.40535783e-23]\n",
            " [9.99844370e-01 1.55629799e-04 1.08589813e-24]\n",
            " [9.99863341e-01 1.36658520e-04 8.08714403e-25]\n",
            " [9.99267845e-01 7.32155322e-04 8.01626543e-23]\n",
            " [9.99638098e-01 3.61901567e-04 9.46427750e-24]\n",
            " [9.99770818e-01 2.29182132e-04 2.90999754e-24]\n",
            " [9.99744454e-01 2.55545632e-04 3.81801900e-24]\n",
            " [9.99164838e-01 8.35161688e-04 1.03424177e-22]\n",
            " [9.99567153e-01 4.32846680e-04 1.90945147e-23]\n",
            " [9.99609774e-01 3.90226179e-04 1.47561171e-23]\n",
            " [9.96369051e-01 3.63094946e-03 6.64107226e-21]\n",
            " [9.99369789e-01 6.30211383e-04 4.86274915e-23]\n",
            " [9.98164221e-01 1.83577888e-03 2.13456744e-21]\n",
            " [9.99030225e-01 9.69775484e-04 3.23291023e-22]\n",
            " [9.98950565e-01 1.04943489e-03 2.35856377e-22]\n",
            " [9.99681894e-01 3.18106330e-04 9.06252836e-24]\n",
            " [9.99343811e-01 6.56189458e-04 5.73488784e-23]\n",
            " [9.99726127e-01 2.73873397e-04 5.51858597e-24]\n",
            " [9.99557989e-01 4.42011403e-04 1.88875081e-23]\n",
            " [4.12766159e-04 9.99556038e-01 3.11958398e-05]\n",
            " [1.53957129e-04 9.99608034e-01 2.38008485e-04]\n",
            " [5.41729176e-05 9.97965837e-01 1.97999015e-03]\n",
            " [4.96343405e-05 9.98854178e-01 1.09618774e-03]\n",
            " [3.36875767e-05 9.95866358e-01 4.09995456e-03]\n",
            " [4.89311422e-05 9.98308615e-01 1.64245386e-03]\n",
            " [3.99706242e-05 9.95971542e-01 3.98848766e-03]\n",
            " [4.32229788e-03 9.95677625e-01 7.71360466e-08]\n",
            " [2.27093096e-04 9.99690033e-01 8.28738249e-05]\n",
            " [8.04035060e-05 9.99376698e-01 5.42898348e-04]\n",
            " [5.17339070e-04 9.99477373e-01 5.28811372e-06]\n",
            " [1.10104073e-04 9.99494271e-01 3.95624855e-04]\n",
            " [7.94263618e-04 9.99202821e-01 2.91504286e-06]\n",
            " [3.10341780e-05 9.95224988e-01 4.74397782e-03]\n",
            " [2.54898492e-03 9.97450569e-01 4.46243827e-07]\n",
            " [5.98012700e-04 9.99388932e-01 1.30551609e-05]\n",
            " [1.99397028e-05 9.88297203e-01 1.16828570e-02]\n",
            " [2.03853570e-03 9.97960908e-01 5.56685596e-07]\n",
            " [4.43114133e-06 8.90733714e-01 1.09261855e-01]\n",
            " [7.72615057e-04 9.99223514e-01 3.87134958e-06]\n",
            " [2.05042558e-06 6.47093088e-01 3.52904861e-01]\n",
            " [8.10096383e-04 9.99184845e-01 5.05818010e-06]\n",
            " [2.65745560e-06 7.67782186e-01 2.32215157e-01]\n",
            " [8.96648736e-05 9.99427480e-01 4.82854991e-04]\n",
            " [5.54600175e-04 9.99432919e-01 1.24804190e-05]\n",
            " [3.53831045e-04 9.99609703e-01 3.64662850e-05]\n",
            " [5.13693963e-05 9.98195137e-01 1.75349362e-03]\n",
            " [4.71997746e-06 8.48915060e-01 1.51080220e-01]\n",
            " [2.88183278e-05 9.94355513e-01 5.61566853e-03]\n",
            " [1.64381057e-02 9.83561889e-01 5.37493915e-09]\n",
            " [6.95017538e-04 9.99300557e-01 4.42517762e-06]\n",
            " [2.31291586e-03 9.97686761e-01 3.23554047e-07]\n",
            " [1.06541728e-03 9.98932148e-01 2.43444388e-06]\n",
            " [3.25631627e-07 2.77357367e-01 7.22642307e-01]\n",
            " [1.41889837e-05 9.78272915e-01 2.17128955e-02]\n",
            " [7.19420159e-05 9.98661948e-01 1.26610976e-03]\n",
            " [8.42621389e-05 9.99123257e-01 7.92480564e-04]\n",
            " [4.14604664e-05 9.98189960e-01 1.76857974e-03]\n",
            " [4.14386252e-04 9.99562974e-01 2.26397009e-05]\n",
            " [9.60369831e-05 9.99576630e-01 3.27333121e-04]\n",
            " [5.36579831e-05 9.98830548e-01 1.11579381e-03]\n",
            " [6.33155634e-05 9.98730858e-01 1.20582626e-03]\n",
            " [4.88207762e-04 9.99499974e-01 1.18182977e-05]\n",
            " [3.66003876e-03 9.96339858e-01 1.03162548e-07]\n",
            " [9.87384603e-05 9.99540022e-01 3.61239949e-04]\n",
            " [6.86665093e-04 9.99305746e-01 7.58882968e-06]\n",
            " [2.32019392e-04 9.99696242e-01 7.17387321e-05]\n",
            " [3.82703466e-04 9.99590629e-01 2.66675624e-05]\n",
            " [1.57127658e-02 9.84287229e-01 5.34172528e-09]\n",
            " [2.51262324e-04 9.99692079e-01 5.66590566e-05]\n",
            " [8.52106343e-12 3.46052891e-04 9.99653947e-01]\n",
            " [4.19426219e-09 1.84680397e-02 9.81531956e-01]\n",
            " [5.22338078e-10 4.47423340e-03 9.95525766e-01]\n",
            " [5.92734997e-09 2.19458138e-02 9.78054180e-01]\n",
            " [9.08269172e-11 1.53009155e-03 9.98469908e-01]\n",
            " [5.08326774e-11 1.03490200e-03 9.98965098e-01]\n",
            " [7.09802844e-08 1.19926789e-01 8.80073140e-01]\n",
            " [1.20810294e-09 7.74265855e-03 9.92257340e-01]\n",
            " [7.74664396e-10 6.34365855e-03 9.93656341e-01]\n",
            " [7.12447658e-11 1.18608053e-03 9.98813919e-01]\n",
            " [1.56131208e-07 1.58370344e-01 8.41629500e-01]\n",
            " [6.37073487e-09 2.34751064e-02 9.76524887e-01]\n",
            " [2.48263261e-09 1.20393224e-02 9.87960675e-01]\n",
            " [7.78198198e-10 6.56663260e-03 9.93433367e-01]\n",
            " [5.21103300e-11 1.13452162e-03 9.98865478e-01]\n",
            " [8.60565685e-10 6.03812537e-03 9.93961874e-01]\n",
            " [3.67839936e-08 6.78134874e-02 9.32186476e-01]\n",
            " [3.31701764e-10 2.98462666e-03 9.97015373e-01]\n",
            " [2.08178664e-12 1.53899031e-04 9.99846101e-01]\n",
            " [1.51681154e-07 1.94257628e-01 8.05742220e-01]\n",
            " [3.22149018e-10 3.21487030e-03 9.96785129e-01]\n",
            " [4.52308030e-09 1.91059233e-02 9.80894072e-01]\n",
            " [4.43909600e-11 9.75382225e-04 9.99024618e-01]\n",
            " [2.76061910e-07 2.45712676e-01 7.54287048e-01]\n",
            " [2.32147745e-09 1.10982029e-02 9.88901795e-01]\n",
            " [2.86624032e-08 5.48840355e-02 9.45115936e-01]\n",
            " [6.87318619e-07 4.04064778e-01 5.95934534e-01]\n",
            " [7.43928531e-07 4.10420471e-01 5.89578785e-01]\n",
            " [2.16366927e-10 2.71773277e-03 9.97282267e-01]\n",
            " [5.02772930e-07 3.25130235e-01 6.74869263e-01]\n",
            " [1.12822604e-09 7.45739126e-03 9.92542608e-01]\n",
            " [3.35481673e-08 5.38703499e-02 9.46129617e-01]\n",
            " [9.06201971e-11 1.57437273e-03 9.98425627e-01]\n",
            " [3.20029245e-06 7.90196164e-01 2.09800636e-01]\n",
            " [9.22723299e-08 1.34080374e-01 8.65919534e-01]\n",
            " [1.19027614e-10 1.73192341e-03 9.98268076e-01]\n",
            " [1.25383477e-10 1.77348188e-03 9.98226518e-01]\n",
            " [4.67592107e-08 7.78477137e-02 9.22152240e-01]\n",
            " [1.06075151e-06 4.92799059e-01 5.07199880e-01]\n",
            " [1.04948854e-08 2.93426973e-02 9.70657292e-01]\n",
            " [9.52014600e-11 1.53272990e-03 9.98467270e-01]\n",
            " [7.22701526e-09 2.29205803e-02 9.77079413e-01]\n",
            " [4.19426219e-09 1.84680397e-02 9.81531956e-01]\n",
            " [9.57114234e-11 1.51968937e-03 9.98480311e-01]\n",
            " [5.48636416e-11 1.06158393e-03 9.98938416e-01]\n",
            " [1.34917960e-09 8.14498082e-03 9.91855018e-01]\n",
            " [1.26239977e-08 3.75567639e-02 9.62443224e-01]\n",
            " [2.47038936e-08 5.21348944e-02 9.47865081e-01]\n",
            " [7.17816772e-10 5.28241508e-03 9.94717584e-01]\n",
            " [9.84430596e-08 1.27846292e-01 8.72153610e-01]]\n",
            "\n",
            "Score\n",
            "0.9866666666666667\n"
          ]
        }
      ],
      "source": [
        "# IRIS MLP\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier # neural network\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "x, y = load_iris(return_X_y=True, as_frame=True) # First param to denote \"dictionary form\", second param to declare as pandas df\n",
        "mlpLearner = MLPClassifier(max_iter = 1000, learning_rate_init = 0.01, batch_size=4, tol=0.001, verbose=False, activation='logistic') # change arguments to customize\n",
        "\n",
        "# Full Data\n",
        "mlpLearner.fit(x,y)\n",
        "predFull = mlpLearner.predict(x)\n",
        "probFull = mlpLearner.predict_proba(x)\n",
        "scoreFull = mlpLearner.score(x,y)\n",
        "\n",
        "print(\"Class Prediction\")\n",
        "print(predFull)\n",
        "print(\"\\nProbability Prediction\")\n",
        "print(probFull)\n",
        "print(\"\\nScore\")\n",
        "print(scoreFull)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TubesB_13519103_13519135_13519141_13519153.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
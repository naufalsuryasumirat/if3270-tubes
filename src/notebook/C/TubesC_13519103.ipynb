{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHqirTMkZlB9"
      },
      "source": [
        "# Tugas Besar IF3270 - Pembelajaran Mesin Bag. C\n",
        "\n",
        "Anggota Kelompok:\n",
        "\n",
        "1. 13519103 - Bryan Rinaldo\n",
        "2. 13519135 - Naufal Alexander Suryasumirat\n",
        "3. 13519141 - Naufal Yahya Kurnianto\n",
        "4. 13519153 - Maximillian Lukman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qDWeaLG-ZafX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVXnXhTzjl5E"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OuiBSjD0jIc8"
      },
      "outputs": [],
      "source": [
        "# Activation functions\n",
        "## Linear\n",
        "linear = lambda x: x\n",
        "linear = np.vectorize(linear)\n",
        "## Sigmoid\n",
        "sigmoid = lambda x: 1 / (1 + math.exp(-x))\n",
        "sigmoid = np.vectorize(sigmoid)\n",
        "## ReLU\n",
        "relu = lambda x: float(max(0, x))\n",
        "relu = np.vectorize(relu)\n",
        "## Softmax\n",
        "softmax = lambda x: np.exp(x) / np.exp(x).sum(axis=0) # already used for vectors\n",
        "## Dict\n",
        "activation_functions = {\n",
        "    'linear': linear,\n",
        "    'sigmoid': sigmoid,\n",
        "    'relu': relu,\n",
        "    'softmax': softmax\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-drdpwjsX4"
      },
      "source": [
        "## Loss/Cost Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vvN5hZ-aiy3o"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "## Linear, Sigmoid, ReLU\n",
        "def general_loss(predict: np.array or int, target: list or int):\n",
        "    if (isinstance(predict, type(int))): return 0.5 * ((target - predict) ** 2)\n",
        "    sum = 0\n",
        "    for i in range(len(target)):\n",
        "        sum += ((target[i] - predict[i]) ** 2)\n",
        "    return 0.5 * sum\n",
        "## Softmax\n",
        "def softmax_loss(predict: np.array, target: int):\n",
        "    return -math.log(predict[target]) # base e\n",
        "## Dict\n",
        "loss_functions = {\n",
        "    'linear': general_loss,\n",
        "    'sigmoid': general_loss,\n",
        "    'relu': general_loss,\n",
        "    'softmax': softmax_loss\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYvuu6z_jxJv"
      },
      "source": [
        "## Back-propagation Functions / Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gd7t1kvCi5hQ"
      },
      "outputs": [],
      "source": [
        "# Back-propagation functions (derivatives)\n",
        "## Linear\n",
        "linear_backprop = lambda x: 1\n",
        "## Sigmoid\n",
        "# sigmoid_backprop = lambda x: sigmoid(x) * (1 - sigmoid(x)) # or x * (1 - x)?\n",
        "sigmoid_backprop = lambda x: x * (1 - x)\n",
        "## ReLU\n",
        "relu_backprop = lambda x: float(x >= 0)\n",
        "relu_backprop = np.vectorize(relu_backprop)\n",
        "## Softmax\n",
        "def softmax_backprop(arr, targ):\n",
        "    arr_copy = np.copy(arr)\n",
        "    arr_copy[targ] = -(1 - arr_copy[targ])\n",
        "    return arr_copy\n",
        "## Dict\n",
        "backprop_functions = {\n",
        "    'linear': linear_backprop,\n",
        "    'sigmoid': sigmoid_backprop,\n",
        "    'relu': relu_backprop,\n",
        "    'softmax': softmax_backprop\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbpFDh600eIP"
      },
      "source": [
        "## Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YNEXM6kWi_8-"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    # n_neuron: number of neuron, weights: weight matrix, activation: activation function\n",
        "    def __init__(self, n_neuron: int, weights: np.array, activation: str) -> None:\n",
        "        self.n_neuron = n_neuron # visualization purposes\n",
        "        self.weights = weights # weights (including bias)\n",
        "        self.activation = activation # activation type [linear, sigmoid, relu, softmax]\n",
        "        self.act_function = activation_functions[activation] # activation function used\n",
        "        self.loss_function = loss_functions[activation] # loss function used\n",
        "        self.backprop_functions = backprop_functions[activation] # derivative activation functions\n",
        "        self.result = [] # retain result of feed forward iteration\n",
        "        self.deltas = np.zeros_like(self.weights) # initialize delta for backprop\n",
        "        # every feed forward add result, every backprop add to delta\n",
        "\n",
        "    def calculate(self, in_matrix: np.array) -> np.array:\n",
        "        self.result = self.act_function(np.dot(self.weights.transpose(), in_matrix))\n",
        "        return self.result # [a0, a1, a2, ..., an]\n",
        "\n",
        "    def calculate_loss(self, prediction: (np.array or int), target: (list or int)) -> float: # used for calculating loss for output layer\n",
        "        if (self.activation == \"softmax\"): return self.loss_function(prediction, np.argmax(target))\n",
        "        return self.loss_function(prediction, target)\n",
        "    \n",
        "    def update_weight(self):\n",
        "        self.weights += self.deltas # adding deltas to weights\n",
        "        self.deltas = np.zeros_like(self.deltas) # resettings deltas for next mini-batch\n",
        "        return self.weights # for verbose purpose\n",
        "    \n",
        "    def add_deltas(self, delta_matrix: np.array) -> None:\n",
        "        self.deltas += delta_matrix\n",
        "        return self.deltas # for verbose purpose\n",
        "    \n",
        "    def get_structure(self) -> tuple((int, np.array, np.array)):\n",
        "        # n_neuron: int, weight matrix: np.array, bias weight matrix: np.array\n",
        "        n_neuron = self.n_neuron\n",
        "        weight_neuron = self.weights[:-1,]\n",
        "        weight_bias = self.weights[-1:,].flatten()\n",
        "        activation = self.activation\n",
        "        return (n_neuron, weight_neuron, weight_bias, activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZK7aIz0pCU"
      },
      "source": [
        "## FFNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M5j2PgKYjE_e"
      },
      "outputs": [],
      "source": [
        "class FFNN:\n",
        "    def __init__(self,  \n",
        "            hidden_layers: list,\n",
        "            input_layer = None,\n",
        "            threshold = 0.5,\n",
        "            learning_rate = 0.01,\n",
        "            err_threshold = 0.001,\n",
        "            max_iter = 5000,\n",
        "            batch_size = 1) -> None:\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.output_layer = hidden_layers[-1]\n",
        "        self.output_activation = self.output_layer.activation\n",
        "        self.input_layer = input_layer\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "        self.err_threshold = err_threshold\n",
        "        self.max_iter = max_iter\n",
        "        self.batch_size = batch_size # default incremental SGD\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_model(input_size: int, n_neurons: list, activations: list):\n",
        "        if (len(n_neurons) != len(activations)): return None\n",
        "        arr = []\n",
        "        for i in range(len(n_neurons)):\n",
        "            if (i == 0): arr.append(Layer(\n",
        "                n_neurons[i], np.random.uniform(low = -1.0, high = 1.0, \n",
        "                    size = (input_size + 1, n_neurons[i])),\n",
        "                    activations[i])\n",
        "                )\n",
        "            else: arr.append(Layer(\n",
        "                n_neurons[i], np.random.uniform(low = -1.0, high = 1.0,\n",
        "                    size = (n_neurons[i - 1] + 1, n_neurons[i])),\n",
        "                    activations[i])\n",
        "                )\n",
        "        return FFNN(arr)\n",
        "\n",
        "    def feed_forward(self) -> (np.array or None):\n",
        "        if (isinstance(self.input_layer, type(None))): return None\n",
        "        if len(self.input_layer.shape) == 1: return self.forward(self.input_layer)\n",
        "        else:\n",
        "            outputs = []\n",
        "            for data in self.input_layer: outputs.append(self.forward(data))\n",
        "            if (self.output_layer.activation == 'softmax'): return outputs\n",
        "            return np.array(outputs).flatten()\n",
        "    \n",
        "    def forward(self, input) -> (np.array or None):\n",
        "        output = input\n",
        "        for i in range(0, len(self.hidden_layers)):\n",
        "            output = self.hidden_layers[i].calculate(np.append(output, 1))\n",
        "        if (self.output_layer.activation == 'softmax'): return output # usually used for multiclass\n",
        "        if (self.output_layer.n_neuron > 1): return np.where(output > self.threshold, 1, 0) # multiclass non-softmax\n",
        "        return int(output > self.threshold) # binary\n",
        "    \n",
        "    def fit(self, x_train, y_train, randomize = False, learning_rate = None, \n",
        "            batch_size = None, max_iter = None, \n",
        "            err_threshold = None, update_every = 250) -> None:\n",
        "        if learning_rate is not None: self.learning_rate = learning_rate\n",
        "        if batch_size is not None: self.batch_size = batch_size\n",
        "        if max_iter is not None: self.max_iter = max_iter\n",
        "        if err_threshold is not None: self.err_threshold = err_threshold\n",
        "        for epoch in range(self.max_iter):\n",
        "            training_data = x_train\n",
        "            training_target = y_train\n",
        "            if randomize:\n",
        "                pass # randomize dataset x_train here\n",
        "            error_sum = 0 # initialize error (for comparing with err_threshold)\n",
        "            for iter in range(len(y_train)):\n",
        "                pred = self.predict(training_data[iter]) # results already encoded\n",
        "                pred = self.output_layer.result # result before encoded\n",
        "                error = self.output_layer.calculate_loss(pred, training_target[iter])\n",
        "                self.backpropagate(training_data[iter], training_target[iter])\n",
        "                error_sum += error\n",
        "                if ((iter + 1) % self.batch_size == 0 or iter == len(training_target) - 1):\n",
        "                    self.update_weights() # update weights (mini-batch)\n",
        "            err_avg = error_sum / len(y_train)\n",
        "\n",
        "            if (err_avg < self.err_threshold):\n",
        "                print(\"Epoch %d, Loss: %.6f | Reason for stopping: err < err_threshold\" % (epoch, err_avg))\n",
        "                break # stop fitting process when avg error < threshold\n",
        "\n",
        "            if (epoch % update_every == 0):\n",
        "                print(\"Epoch %d, Loss: %.6f\" % (1 if epoch == 0 else epoch, err_avg))\n",
        "        return\n",
        "    \n",
        "    def backpropagate(self, input, target): # update deltas for every layer\n",
        "        err_term = 0\n",
        "        for iter in reversed(range(0, len(self.hidden_layers))):\n",
        "            prev_layer = None if iter == 0 else self.hidden_layers[iter - 1]\n",
        "            prev_result = np.atleast_2d(np.append(input, 1)) if prev_layer == None \\\n",
        "                else np.atleast_2d(np.append(prev_layer.result, 1))\n",
        "            if (iter == len(self.hidden_layers) - 1): # if output layer\n",
        "                if (self.output_activation == \"softmax\"): # if softmax output layer\n",
        "                    pred = self.output_layer.result\n",
        "                    err_deriv = self.output_layer.backprop_functions(pred, np.argmax(target))\n",
        "                    err_term = err_deriv\n",
        "                    gradient = np.dot(prev_result.T,\n",
        "                        np.atleast_2d(err_deriv))\n",
        "                    delta = -self.learning_rate * gradient\n",
        "                    self.output_layer.add_deltas(delta)\n",
        "                    pass\n",
        "                else: # if other output layer\n",
        "                    pred = self.output_layer.result\n",
        "                    err_deriv = -(np.array(target) - pred)\n",
        "                    err_term = err_deriv\n",
        "                    donet = self.output_layer.backprop_functions(pred)\n",
        "                    gradient = np.dot(prev_result.T,\n",
        "                        np.atleast_2d(err_deriv * donet))\n",
        "                    delta = -self.learning_rate * gradient\n",
        "                    self.output_layer.add_deltas(delta)\n",
        "            else: # if hidden layer\n",
        "                this_layer = self.hidden_layers[iter]\n",
        "                next_layer = self.hidden_layers[iter + 1]\n",
        "                err_term = np.add.reduce(next_layer.weights[:-1].T * \n",
        "                    np.atleast_2d(err_term).T, 0) / np.shape(err_term)[0]\n",
        "                donet = this_layer.backprop_functions(this_layer.result) # no softmax in hidden layer\n",
        "                gradient = np.dot(prev_result.T,\n",
        "                    np.atleast_2d(err_term * donet))\n",
        "                delta = -self.learning_rate * gradient\n",
        "                self.hidden_layers[iter].add_deltas(delta)\n",
        "                pass\n",
        "        return\n",
        "        \n",
        "    def update_weights(self):\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_weight()\n",
        "        return\n",
        "    \n",
        "    def attach_input(self, input_layer: np.array) -> None:\n",
        "        self.input_layer = input_layer\n",
        "        return\n",
        "\n",
        "    def attach_hidden_layer(self, hidden_layer: Layer) -> None:\n",
        "        self.hidden_layers.append(hidden_layer)\n",
        "        return\n",
        "\n",
        "    def predict(self, input_layer: np.array) -> list: # input_layer without bias\n",
        "        self.input_layer = input_layer\n",
        "        return self.feed_forward()\n",
        "\n",
        "    def get_structure(self) -> tuple((np.array, list)):\n",
        "        return (self.input_layer, [layer.get_structure() for layer in self.hidden_layers])\n",
        "    def save(self) -> None:\n",
        "      with open(\"model.txt\", \"w\") as f:\n",
        "        n_layer_save = len(self.get_structure()[1])\n",
        "        f.writelines(str(n_layer_save+1))\n",
        "        f.writelines('\\n')\n",
        "\n",
        "        #initial neuron\n",
        "        f.writelines(str(len(self.get_structure()[1][0][1])))\n",
        "        f.writelines('\\n')\n",
        "\n",
        "        for i in range(n_layer_save):\n",
        "          \n",
        "          # BIAS\n",
        "          for bias in self.get_structure()[1][i][2]:\n",
        "            f.write(\"%s \" % bias)\n",
        "          f.write('\\n')\n",
        "\n",
        "          # WEIGHT\n",
        "          countLayer = len(self.get_structure()[1][i][1])\n",
        "          for j in range(countLayer):\n",
        "            for weight in self.get_structure()[1][i][1][j]:\n",
        "              f.write(\"%s \" % weight)\n",
        "            f.write('\\n')\n",
        "\n",
        "          # ACTIVATION\n",
        "          f.write(str(self.get_structure()[1][i][3]))\n",
        "          f.write('\\n')\n",
        "\n",
        "          # NEURON\n",
        "          f.write(str(self.get_structure()[1][i][0]))\n",
        "          f.write('\\n')\n",
        "      return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3J3Ew10rPI"
      },
      "source": [
        "## Accuracy Calculation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vQ1rchygjU8g"
      },
      "outputs": [],
      "source": [
        "# Fungsi menghitung akurasi dari model\n",
        "def calculate_accuracy(model: FFNN, input_set, validation_set: list, is_softmax = False):\n",
        "    # returns range from 1..100 (percentage)\n",
        "    predicted_set = model.predict(np.array(input_set))\n",
        "    if (not isinstance(predicted_set, (list, np.ndarray))): return int(predicted_set == validation_set[0]) * 100\n",
        "    if (len(predicted_set) != len(validation_set)): return None\n",
        "    num_correct = 0\n",
        "    for i in range(len(predicted_set)):\n",
        "        if is_softmax:\n",
        "            if (np.argmax(predicted_set[i]) == np.argmax(validation_set[i])): num_correct += 1\n",
        "        else:\n",
        "            if predicted_set[i].tolist() == validation_set[i].tolist(): num_correct += 1\n",
        "    return num_correct / len(validation_set) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgJ5O7DV0vbd"
      },
      "source": [
        "## Input Model File Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ytp347DRjFwn"
      },
      "outputs": [],
      "source": [
        "# Fungsi membaca input file\n",
        "def _input(filename: str, with_input = False) -> tuple((FFNN, np.array, np.array)):\n",
        "    f = open(filename, \"r\")\n",
        "    f = f.readlines()\n",
        "    f = [line.strip() for line in f]\n",
        "\n",
        "    nLayer = int(f[0])\n",
        "    f = f[1:]\n",
        "    n_layer_neurons = []\n",
        "    struct_model = {}\n",
        "\n",
        "    for i in range(nLayer-1):\n",
        "        struct_model[i] = {}\n",
        "        n_layer_neurons.append(int(f[0]))\n",
        "        struct_model[i][\"b\"] = [float(b) for b in f[1].split()]\n",
        "        struct_model[i][\"w\"] = [[float(w) for w in weights.split()] for weights in f[2:(2 + int(f[0]))]]\n",
        "        struct_model[i][\"f\"] = f[2 + int(f[0])]\n",
        "        f = f[2 + int(f[0]) + 1:]\n",
        "\n",
        "    n_layer_neurons.append(int(f[0]))\n",
        "    \n",
        "    if (with_input):\n",
        "        n_input = int(f[1])\n",
        "        f = f[2:]\n",
        "        input_data = []\n",
        "        for i in range(n_input):\n",
        "            input = [int(x) for x in (f[i].split())]\n",
        "            input_data.append(input)\n",
        "\n",
        "        f = f[n_input:]\n",
        "        validation_data = []\n",
        "        for i in range(n_input):\n",
        "            result = [int(y) for y in (f[i].split())]\n",
        "            validation_data.append(result)\n",
        "\n",
        "    model_layers = []\n",
        "    for i in range (nLayer-1):\n",
        "        weight = struct_model[i][\"w\"]\n",
        "        weight.append(struct_model[i][\"b\"])\n",
        "        layer = Layer(n_layer_neurons[i+1], np.array(weight), struct_model[i][\"f\"].lower())\n",
        "        model_layers.append(layer)\n",
        "    \n",
        "    if (with_input):\n",
        "        return FFNN(model_layers, np.array(input_data)), input_data, validation_data\n",
        "    else:\n",
        "        return FFNN(model_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ww-1Adh0yEA"
      },
      "source": [
        "## Show Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rz8N1pNYjPf0"
      },
      "outputs": [],
      "source": [
        "# Memperlihatkan koefisien dan struktur dari model\n",
        "def showModel(model: FFNN): #masukan berupa FFNN\n",
        "    initLayers = model.get_structure()\n",
        "    countLayer = len(initLayers[1])\n",
        "    print(\"==============Model FFNN==============\\n\")\n",
        "    print(\"------------------------------\")\n",
        "    for j in range(0, countLayer):\n",
        "        weight = initLayers[1][j][1]\n",
        "        bias = initLayers[1][j][2]\n",
        "\n",
        "        if (j == (countLayer - 1)):\n",
        "            print(\"------ Output Layer ------\" )\n",
        "            print(\"Weight: \" , weight)\n",
        "            print(\"Bias: \" , bias)\n",
        "            print('\\n')\n",
        "            print(\"------------------------------\")\n",
        "        else:\n",
        "            print(\"--- Hidden Layer %d ---\" %(j+1))\n",
        "            print(\"H%d Weight: \" %(j+1), weight)\n",
        "            print(\"H%d Bias: \" %(j+1), bias)\n",
        "            print('\\n')\n",
        "            print(\"------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYPhpLIo4w2"
      },
      "source": [
        "## Fungsi Formatting Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4ef3O3I-3Aq-"
      },
      "outputs": [],
      "source": [
        "def input_dataset(dataset_input):\n",
        "  x_iris = dataset_input.data\n",
        "  y_iris = dataset_input.target\n",
        "\n",
        "  y_iris_temp = [] \n",
        "  maxElement = np.amax(y_iris)\n",
        "  for x in range(len(y_iris)):\n",
        "    iris = []\n",
        "    i = 0\n",
        "\n",
        "    # CEK UDH FULL\n",
        "    while len(iris) < (maxElement+1) :\n",
        "      if(i == y_iris[x]):\n",
        "        iris.append(1)\n",
        "      else:\n",
        "        iris.append(0)\n",
        "      i+=1\n",
        "    y_iris_temp.append(iris)\n",
        "\n",
        "  y_iris_return = np.array(y_iris_temp)\n",
        "  returnDict = {\n",
        "      \"x_train\" : x_iris,\n",
        "      \"y_train\" : y_iris_return\n",
        "  }\n",
        "  return returnDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ea4KYXz59D"
      },
      "source": [
        "## Fit Kelas FFNN (backprop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyJ0Li7R0FHE"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aSprbANjdg8",
        "outputId": "c0d7f858-fec4-4262-b314-000ace301a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Iris\n",
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                  5.1               3.5                1.4               0.2\n",
            "1                  4.9               3.0                1.4               0.2\n",
            "2                  4.7               3.2                1.3               0.2\n",
            "3                  4.6               3.1                1.5               0.2\n",
            "4                  5.0               3.6                1.4               0.2\n",
            "..                 ...               ...                ...               ...\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n",
            "\n",
            "[150 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "dataIris = datasets.load_iris()\n",
        "df = pd.DataFrame(data=dataIris.data, columns=dataIris.feature_names)\n",
        "\n",
        "print(\"Data Iris\")\n",
        "print(df)\n",
        "\n",
        "dataset = input_dataset(dataIris) # using input_dataset function\n",
        "\n",
        "x_train = dataset['x_train']\n",
        "y_train = dataset['y_train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d6BRF5O4eiK"
      },
      "source": [
        "## Defining FFNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fungsi FFNN.generate_model\n",
        "\n",
        "Digunakan untuk mendefinisikan model awal yang akan digunakan untuk training (random weight diantara -1.0 dan 1.0)"
      ],
      "metadata": {
        "id": "SOs6E3RyBJfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "input_size: size atau panjang dari input data / x_train\n",
        "\n",
        "n_neurons: banyak neuron untuk tiap layer\n",
        "\n",
        "activations: fungsi aktivasi untuk setiap layer"
      ],
      "metadata": {
        "id": "P3dTBwMDA4ZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9eLL_s_Q0Jw2"
      },
      "outputs": [],
      "source": [
        "ffnn_model = FFNN.generate_model(\n",
        "    4, [6, 4, 5, 3], ['sigmoid', 'relu', 'linear', 'softmax']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krZBFEJ24wwS"
      },
      "source": [
        "## Model Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-OtUuKH4oaq",
        "outputId": "efc782c3-f140-4efd-b911-59edd2bb6202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Model FFNN==============\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 1 ---\n",
            "H1 Weight:  [[ 0.34946984  0.27410438  0.15458445 -0.04498611 -0.0901094  -0.14554413]\n",
            " [ 0.93971333 -0.26713782  0.04344315  0.64011259 -0.12456725 -0.64132777]\n",
            " [ 0.12749986  0.733249   -0.0121222  -0.31632702  0.65377666  0.58979344]\n",
            " [ 0.3482548  -0.91214758  0.29261866 -0.84104137 -0.90729138 -0.79656163]]\n",
            "H1 Bias:  [ 0.92497263  0.74289731  0.53171339 -0.9272321   0.26953848 -0.38956392]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 2 ---\n",
            "H2 Weight:  [[-0.5622343  -0.22830746 -0.29305007 -0.00358271]\n",
            " [ 0.79577188 -0.76972573 -0.95545282 -0.83618171]\n",
            " [-0.05578808  0.07422817 -0.06237316  0.52816116]\n",
            " [ 0.40612959  0.65568235  0.97817598 -0.02989557]\n",
            " [-0.87577106 -0.29758202 -0.97162304  0.2998455 ]\n",
            " [ 0.08057758  0.33305817  0.31964299  0.97088272]]\n",
            "H2 Bias:  [-0.85770576  0.02196974  0.0886889   0.88751857]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 3 ---\n",
            "H3 Weight:  [[ 3.19473355e-02 -3.14646046e-01 -3.31631373e-01  8.00264631e-01\n",
            "  -6.62048319e-01]\n",
            " [ 7.62428302e-01 -9.08288407e-01  4.54756218e-01 -3.39566237e-01\n",
            "  -3.80702920e-01]\n",
            " [ 7.69557407e-04  2.95179477e-01  4.03644561e-01 -7.19128894e-01\n",
            "   6.45298962e-01]\n",
            " [ 6.29868420e-01 -6.43825532e-01  7.74066685e-01 -6.08367678e-01\n",
            "  -2.05903143e-01]]\n",
            "H3 Bias:  [-0.22624078  0.8211485  -0.26603395 -0.51817362 -0.91943109]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "------ Output Layer ------\n",
            "Weight:  [[-0.22590862 -0.06730742 -0.16293808]\n",
            " [ 0.26586461  0.6806552   0.88646297]\n",
            " [-0.81010373  0.12244776 -0.51412626]\n",
            " [ 0.77880355 -0.67989359 -0.65249169]\n",
            " [ 0.41311095 -0.16459177  0.5690517 ]]\n",
            "Bias:  [ 0.12319043 -0.39213734 -0.93519847]\n",
            "\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "showModel(ffnn_model) # showing model before training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZ1_5-S4ydU"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "x_train: data untuk training\n",
        "\n",
        "y_train: target untuk training\n",
        "\n",
        "learning_rate: learning rate untuk dikalikan dengan gradien\n",
        "\n",
        "batch_size: size mini-batch\n",
        "\n",
        "err_threshold: threshold error, kasus berhenti jika error < error threshold\n",
        "\n",
        "max_iter: iterasi maksimal (epoch)\n",
        "\n",
        "update_every: memberikan update setiap x iterasi"
      ],
      "metadata": {
        "id": "radOL5dpAajc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp-yTIIB40MR",
        "outputId": "1b6d80dc-74a6-4234-8c67-b2bf3a9c212e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.148503\n",
            "Epoch 250, Loss: 0.045915\n",
            "Epoch 500, Loss: 0.049897\n",
            "Epoch 750, Loss: 0.050231\n"
          ]
        }
      ],
      "source": [
        "ffnn_model.fit(x_train, y_train, learning_rate = 0.01, batch_size = 4, err_threshold = 0.01, max_iter = 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tgMJrSx5Cqf"
      },
      "source": [
        "## Model After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKH5foKY44_b",
        "outputId": "8ec2249d-2bd9-41a3-ce0b-0b7baeeeb968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Model FFNN==============\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 1 ---\n",
            "H1 Weight:  [[ 0.37084848  0.56201544 -0.2931086   0.51863216 -0.23086801 -0.24467107]\n",
            " [ 0.9479971  -0.05011123 -0.01965936  0.67227434 -0.21525266 -1.08974467]\n",
            " [ 0.13641248  0.72424731  0.29879072 -0.80219975  1.03432402  1.77408187]\n",
            " [ 0.34926692 -0.9526872   0.67570248 -1.51158562 -0.61626672 -0.03372059]]\n",
            "H1 Bias:  [ 0.92853024  0.80470458  0.41154295 -0.77585063  0.21808172 -0.46979416]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 2 ---\n",
            "H2 Weight:  [[-0.38756216 -0.51373486 -0.52732304 -1.08859591]\n",
            " [ 0.92251295 -1.00936113 -1.14894347 -1.81689639]\n",
            " [-0.34309958  0.25964261  0.0070495   1.09369377]\n",
            " [ 1.6839949  -0.59137883  0.16788834 -3.93095253]\n",
            " [-1.20871476 -0.13172877 -0.8946241   0.5914171 ]\n",
            " [-1.06721181  1.23125877  0.8051496   3.64097342]]\n",
            "H2 Bias:  [-0.67729696 -0.26863181 -0.14928566 -0.21499934]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "--- Hidden Layer 3 ---\n",
            "H3 Weight:  [[-4.79226568e-01 -2.88533028e-01 -1.47065924e+00  2.27570942e+00\n",
            "   9.21300636e-02]\n",
            " [ 7.62428302e-01 -9.08288407e-01  4.54756218e-01 -3.39566237e-01\n",
            "  -3.80702920e-01]\n",
            " [ 7.69557407e-04  2.95179477e-01  4.03644561e-01 -7.19128894e-01\n",
            "   6.45298962e-01]\n",
            " [ 1.62610460e+00 -1.75669810e+00  2.41498830e+00 -1.97799438e+00\n",
            "   5.25190152e-01]]\n",
            "H3 Bias:  [-0.92155094  1.87107288 -0.97832389 -0.04658087 -2.27168175]\n",
            "\n",
            "\n",
            "------------------------------\n",
            "------ Output Layer ------\n",
            "Weight:  [[-2.04692402 -0.50446331  2.09523321]\n",
            " [ 1.53340471  2.40616708 -2.10658901]\n",
            " [-4.13102297  0.02455025  2.90469049]\n",
            " [ 4.19044469 -2.02274931 -2.72127712]\n",
            " [ 0.89481581 -2.22214844  2.1449035 ]]\n",
            "Bias:  [ 0.08622102  1.5442744  -2.8346408 ]\n",
            "\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "showModel(ffnn_model) # showing model after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AziE7a3C5KrW"
      },
      "source": [
        "## Predictions After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMRa97Mv5If3",
        "outputId": "765141fb-a7c9-4dee-d619-aad74f9c558b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.99991802e-01 8.19783757e-06 9.54956400e-20]\n",
            "[9.99846294e-01 1.53705577e-04 3.71810746e-18]\n",
            "[9.99959616e-01 4.03844199e-05 7.00055221e-19]\n",
            "[9.98890520e-01 1.10948013e-03 4.39385443e-17]\n",
            "[9.99992101e-01 7.89930630e-06 9.11710907e-20]\n",
            "[9.99984340e-01 1.56603711e-05 2.14368353e-19]\n",
            "[9.99911922e-01 8.80777236e-05 1.85442498e-18]\n",
            "[9.99949471e-01 5.05291029e-05 9.26239066e-19]\n",
            "[9.97668811e-01 2.33118933e-03 1.11126807e-16]\n",
            "[9.99728760e-01 2.71240269e-04 7.55941165e-18]\n",
            "[9.99995896e-01 4.10446900e-06 4.02389093e-20]\n",
            "[9.99654925e-01 3.45074601e-04 1.02122009e-17]\n",
            "[9.99804278e-01 1.95722286e-04 5.02851037e-18]\n",
            "[9.99957317e-01 4.26830948e-05 7.50184053e-19]\n",
            "[9.99999952e-01 4.83733458e-08 1.56754081e-22]\n",
            "[9.99999787e-01 2.12837963e-07 9.97837819e-22]\n",
            "[9.99999630e-01 3.69608326e-07 1.98839864e-21]\n",
            "[9.99991190e-01 8.80981879e-06 1.04483019e-19]\n",
            "[9.99991891e-01 8.10917070e-06 9.42070374e-20]\n",
            "[9.99992230e-01 7.77019712e-06 8.93132998e-20]\n",
            "[9.99883823e-01 1.16177090e-04 2.62086027e-18]\n",
            "[9.99987475e-01 1.25251449e-05 1.62164569e-19]\n",
            "[9.99999283e-01 7.17478833e-07 4.55386934e-21]\n",
            "[9.98789382e-01 1.21061816e-03 4.89991963e-17]\n",
            "[9.83102699e-01 1.68973005e-02 1.32462319e-15]\n",
            "[9.98682893e-01 1.31710690e-03 5.44429278e-17]\n",
            "[9.99775630e-01 2.24370119e-04 5.96424839e-18]\n",
            "[9.99983556e-01 1.64439018e-05 2.27850005e-19]\n",
            "[9.99991423e-01 8.57679853e-06 1.01041980e-19]\n",
            "[9.98495153e-01 1.50484667e-03 6.43071486e-17]\n",
            "[9.98263943e-01 1.73605702e-03 7.68828307e-17]\n",
            "[9.99985285e-01 1.47149094e-05 1.98323620e-19]\n",
            "[9.99997783e-01 2.21667059e-06 1.86377546e-20]\n",
            "[9.99999702e-01 2.97556885e-07 1.51654912e-21]\n",
            "[9.99685861e-01 3.14138558e-04 9.08145937e-18]\n",
            "[9.99995247e-01 4.75311461e-06 4.83339279e-20]\n",
            "[9.99999107e-01 8.93033131e-07 5.98598103e-21]\n",
            "[9.99989333e-01 1.06666184e-05 1.32681669e-19]\n",
            "[9.99637990e-01 3.62009512e-04 1.08421387e-17]\n",
            "[9.99964798e-01 3.52022115e-05 5.89684544e-19]\n",
            "[9.99995441e-01 4.55861412e-06 4.58757626e-20]\n",
            "[9.69862418e-01 3.01375817e-02 2.73837850e-15]\n",
            "[9.99868385e-01 1.31615028e-04 3.06293523e-18]\n",
            "[9.99763763e-01 2.36236528e-04 6.36089836e-18]\n",
            "[9.99257434e-01 7.42565647e-04 2.66042144e-17]\n",
            "[9.99709719e-01 2.90281228e-04 8.22810558e-18]\n",
            "[9.99979535e-01 2.04653344e-05 2.99466762e-19]\n",
            "[9.99811184e-01 1.88815951e-04 4.80781579e-18]\n",
            "[9.99994384e-01 5.61613368e-06 5.95352284e-20]\n",
            "[9.99974177e-01 2.58234217e-05 4.00424158e-19]\n",
            "[2.97606962e-06 9.99997021e-01 2.58918567e-09]\n",
            "[2.93129996e-08 9.99999637e-01 3.33966175e-07]\n",
            "[1.45554420e-10 9.99911484e-01 8.85162256e-05]\n",
            "[3.05307682e-11 9.99542784e-01 4.57216257e-04]\n",
            "[2.13783333e-11 9.99335158e-01 6.64841651e-04]\n",
            "[6.78207598e-12 9.97782949e-01 2.21705050e-03]\n",
            "[4.12391944e-11 9.99666654e-01 3.33345915e-04]\n",
            "[6.86609836e-03 9.93133902e-01 7.40850568e-13]\n",
            "[3.47010503e-08 9.99999686e-01 2.79655691e-07]\n",
            "[8.51850597e-10 9.99986195e-01 1.38037885e-05]\n",
            "[2.43786799e-07 9.99999720e-01 3.59826548e-08]\n",
            "[9.59627651e-09 9.99998910e-01 1.08089453e-06]\n",
            "[2.78448473e-06 9.99997213e-01 2.77689033e-09]\n",
            "[2.54606213e-12 9.93837148e-01 6.16285205e-03]\n",
            "[6.86609836e-03 9.93133902e-01 7.40850568e-13]\n",
            "[4.25391283e-05 9.99957461e-01 1.57811388e-10]\n",
            "[5.94294871e-13 9.72755541e-01 2.72444587e-02]\n",
            "[1.40415575e-04 9.99859584e-01 4.49320423e-11]\n",
            "[5.79764460e-15 4.06753195e-01 5.93246805e-01]\n",
            "[3.64370688e-06 9.99996354e-01 2.09270706e-09]\n",
            "[2.31730643e-16 9.58803005e-02 9.04119700e-01]\n",
            "[1.44059081e-04 9.99855941e-01 4.37372211e-11]\n",
            "[1.50886976e-16 7.77007094e-02 9.22299291e-01]\n",
            "[6.90926667e-11 9.99806230e-01 1.93770110e-04]\n",
            "[6.48913409e-06 9.99993510e-01 1.14045361e-09]\n",
            "[1.81356186e-06 9.99998182e-01 4.35932336e-09]\n",
            "[8.09391907e-11 9.99835931e-01 1.64068743e-04]\n",
            "[1.26265137e-14 5.37135900e-01 4.62864100e-01]\n",
            "[5.58858910e-12 9.97285147e-01 2.71485262e-03]\n",
            "[6.86609836e-03 9.93133902e-01 7.40850568e-13]\n",
            "[2.70523340e-06 9.99997292e-01 2.86252019e-09]\n",
            "[9.75223870e-04 9.99024776e-01 5.84141161e-12]\n",
            "[1.45195585e-04 9.99854804e-01 4.33771085e-11]\n",
            "[4.87551182e-20 1.31184961e-03 9.98688150e-01]\n",
            "[1.03936064e-13 8.65753527e-01 1.34246473e-01]\n",
            "[1.04486526e-09 9.99988863e-01 1.11354748e-05]\n",
            "[1.26622796e-09 9.99990901e-01 9.09778365e-06]\n",
            "[2.22960754e-11 9.99363873e-01 6.36126571e-04]\n",
            "[7.22663490e-07 9.99999266e-01 1.14740837e-08]\n",
            "[5.59626374e-10 9.99978526e-01 2.14738352e-05]\n",
            "[5.02006565e-12 9.96962860e-01 3.03714018e-03]\n",
            "[6.31481497e-11 9.99787007e-01 2.12992902e-04]\n",
            "[1.05090685e-06 9.99998941e-01 7.73863594e-09]\n",
            "[6.86609836e-03 9.93133902e-01 7.40850568e-13]\n",
            "[3.23652411e-10 9.99961802e-01 3.81974735e-05]\n",
            "[2.06946840e-06 9.99997927e-01 3.79422034e-09]\n",
            "[2.36273660e-08 9.99999557e-01 4.18985301e-07]\n",
            "[5.63267440e-07 9.99999422e-01 1.49123538e-08]\n",
            "[6.86609836e-03 9.93133902e-01 7.40850568e-13]\n",
            "[5.72607996e-08 9.99999778e-01 1.65135431e-07]\n",
            "[3.95419449e-28 9.34405702e-08 9.99999907e-01]\n",
            "[1.44640614e-23 2.04059285e-05 9.99979594e-01]\n",
            "[1.59735934e-24 6.59530870e-06 9.99993405e-01]\n",
            "[1.78093779e-23 2.27026139e-05 9.99977297e-01]\n",
            "[2.78672512e-26 8.27721294e-07 9.99999172e-01]\n",
            "[2.88176736e-26 8.42074262e-07 9.99999158e-01]\n",
            "[1.14142848e-21 1.91535933e-04 9.99808464e-01]\n",
            "[5.78526017e-24 1.27570458e-05 9.99987243e-01]\n",
            "[1.44962360e-24 6.27522662e-06 9.99993725e-01]\n",
            "[5.54601736e-26 1.17787970e-06 9.99998822e-01]\n",
            "[5.39159941e-19 4.48987655e-03 9.95510123e-01]\n",
            "[8.48618489e-23 5.05431304e-05 9.99949457e-01]\n",
            "[4.09698773e-23 3.47975805e-05 9.99965202e-01]\n",
            "[1.21537105e-24 5.73310610e-06 9.99994267e-01]\n",
            "[5.17686199e-26 1.13701440e-06 9.99998863e-01]\n",
            "[1.11849433e-23 1.78862394e-05 9.99982114e-01]\n",
            "[1.27813532e-21 2.02970130e-04 9.99797030e-01]\n",
            "[4.55602625e-25 3.46696524e-06 9.99996533e-01]\n",
            "[1.78111999e-28 6.20840927e-08 9.99999938e-01]\n",
            "[1.04092958e-20 5.94662444e-04 9.99405338e-01]\n",
            "[1.02184501e-24 5.24538316e-06 9.99994755e-01]\n",
            "[3.97985541e-23 3.42839960e-05 9.99965716e-01]\n",
            "[3.28337779e-26 9.00319560e-07 9.99999100e-01]\n",
            "[5.93706484e-19 4.71673678e-03 9.95283263e-01]\n",
            "[1.33334773e-23 1.95720755e-05 9.99980428e-01]\n",
            "[2.19684014e-21 2.67916076e-04 9.99732084e-01]\n",
            "[9.62815651e-18 1.95311811e-02 9.80468819e-01]\n",
            "[5.49175321e-18 1.46817718e-02 9.85318228e-01]\n",
            "[1.41295623e-25 1.90240063e-06 9.99998098e-01]\n",
            "[2.01634309e-18 8.80986534e-03 9.91190135e-01]\n",
            "[9.88772562e-24 1.67909277e-05 9.99983209e-01]\n",
            "[1.41020133e-20 6.94774503e-04 9.99305225e-01]\n",
            "[4.02673622e-26 9.99612724e-07 9.99999000e-01]\n",
            "[1.38787183e-16 7.45648140e-02 9.25435186e-01]\n",
            "[1.09013195e-21 1.87074365e-04 9.99812926e-01]\n",
            "[4.02351189e-25 3.25295144e-06 9.99996747e-01]\n",
            "[9.94772444e-26 1.58918886e-06 9.99998411e-01]\n",
            "[1.46013132e-21 2.17303404e-04 9.99782697e-01]\n",
            "[2.29415454e-17 3.03170812e-02 9.69682919e-01]\n",
            "[1.50075613e-21 2.20381658e-04 9.99779618e-01]\n",
            "[1.35622545e-25 1.86285436e-06 9.99998137e-01]\n",
            "[1.29608393e-20 6.65370338e-04 9.99334630e-01]\n",
            "[1.44640614e-23 2.04059285e-05 9.99979594e-01]\n",
            "[5.01534280e-26 1.11868856e-06 9.99998881e-01]\n",
            "[4.85044849e-26 1.09968055e-06 9.99998900e-01]\n",
            "[9.04857040e-23 5.22332844e-05 9.99947767e-01]\n",
            "[7.12924531e-22 1.50478982e-04 9.99849521e-01]\n",
            "[3.91930521e-21 3.60461144e-04 9.99639539e-01]\n",
            "[3.29881104e-24 9.56506865e-06 9.99990435e-01]\n",
            "[6.08341438e-21 4.51563517e-04 9.99548436e-01]\n"
          ]
        }
      ],
      "source": [
        "for data in x_train:\n",
        "    print(ffnn_model.predict(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBshra805hHB"
      },
      "source": [
        "## Accuracy of Model After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt96CwzI2ra8",
        "outputId": "a104ca74-1489-471f-96c7-f95e245d6d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Model: 97.33%\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of Model: %.2f\" % (calculate_accuracy(ffnn_model, x_train, y_train, is_softmax = True)) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqn-Czs-uOdj",
        "outputId": "d3ce240f-1d74-417c-c77d-ee2d57d7e8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Prediction\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
            " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "\n",
            "Probability Prediction\n",
            "[[9.98982165e-01 1.01783460e-03 8.54158164e-18]\n",
            " [9.97913134e-01 2.08686626e-03 5.37779141e-17]\n",
            " [9.98510246e-01 1.48975367e-03 2.22033419e-17]\n",
            " [9.97516944e-01 2.48305636e-03 9.76303176e-17]\n",
            " [9.99056614e-01 9.43385835e-04 7.21360905e-18]\n",
            " [9.98603312e-01 1.39668790e-03 3.36178860e-17]\n",
            " [9.98175532e-01 1.82446835e-03 5.05552917e-17]\n",
            " [9.98621920e-01 1.37808046e-03 2.03251327e-17]\n",
            " [9.96884747e-01 3.11525286e-03 1.65107347e-16]\n",
            " [9.98363426e-01 1.63657423e-03 2.61510533e-17]\n",
            " [9.99162350e-01 8.37650384e-04 5.34358833e-18]\n",
            " [9.98245424e-01 1.75457583e-03 4.25668072e-17]\n",
            " [9.98331356e-01 1.66864364e-03 2.56963144e-17]\n",
            " [9.98692372e-01 1.30762844e-03 1.19987211e-17]\n",
            " [9.99600313e-01 3.99686623e-04 6.09045141e-19]\n",
            " [9.99426773e-01 5.73227224e-04 2.76400395e-18]\n",
            " [9.99194027e-01 8.05972688e-04 5.97848947e-18]\n",
            " [9.98708957e-01 1.29104282e-03 1.88514533e-17]\n",
            " [9.98898323e-01 1.10167675e-03 1.42032101e-17]\n",
            " [9.98954522e-01 1.04547807e-03 1.20436396e-17]\n",
            " [9.98423633e-01 1.57636732e-03 3.08541069e-17]\n",
            " [9.98498019e-01 1.50198091e-03 3.66422968e-17]\n",
            " [9.99352764e-01 6.47235559e-04 2.17585349e-18]\n",
            " [9.94967026e-01 5.03297367e-03 1.15700481e-15]\n",
            " [9.97095559e-01 2.90444140e-03 1.97333343e-16]\n",
            " [9.97205836e-01 2.79416417e-03 1.30290700e-16]\n",
            " [9.97217549e-01 2.78245076e-03 1.95473884e-16]\n",
            " [9.98873174e-01 1.12682567e-03 1.17362874e-17]\n",
            " [9.98899943e-01 1.10005721e-03 1.01514227e-17]\n",
            " [9.97572978e-01 2.42702208e-03 9.84739379e-17]\n",
            " [9.97330672e-01 2.66932781e-03 1.22025611e-16]\n",
            " [9.98056916e-01 1.94308415e-03 6.55152140e-17]\n",
            " [9.99505905e-01 4.94094951e-04 1.23117613e-18]\n",
            " [9.99551211e-01 4.48789448e-04 1.00578924e-18]\n",
            " [9.97857503e-01 2.14249695e-03 6.28011641e-17]\n",
            " [9.98866058e-01 1.13394174e-03 9.63464115e-18]\n",
            " [9.99225943e-01 7.74056704e-04 3.64206707e-18]\n",
            " [9.99215937e-01 7.84063077e-04 3.83278859e-18]\n",
            " [9.97749328e-01 2.25067186e-03 6.67842046e-17]\n",
            " [9.98677755e-01 1.32224473e-03 1.79154554e-17]\n",
            " [9.98837511e-01 1.16248933e-03 1.35910328e-17]\n",
            " [9.90654193e-01 9.34580728e-03 2.86091619e-15]\n",
            " [9.98295252e-01 1.70474837e-03 3.33130742e-17]\n",
            " [9.95607368e-01 4.39263188e-03 9.49151266e-16]\n",
            " [9.97501822e-01 2.49817774e-03 1.88251182e-16]\n",
            " [9.97078167e-01 2.92183339e-03 1.57587080e-16]\n",
            " [9.99046113e-01 9.53887084e-04 8.58164256e-18]\n",
            " [9.98174971e-01 1.82502917e-03 4.12578984e-17]\n",
            " [9.99132823e-01 8.67177399e-04 5.95383082e-18]\n",
            " [9.98655813e-01 1.34418704e-03 1.75143792e-17]\n",
            " [1.17698382e-03 9.97049999e-01 1.77301692e-03]\n",
            " [5.97090394e-04 9.93219551e-01 6.18335895e-03]\n",
            " [2.18439379e-04 9.67670786e-01 3.21107744e-02]\n",
            " [2.17473369e-04 9.81780506e-01 1.80020209e-02]\n",
            " [1.45852125e-04 9.47833456e-01 5.20206915e-02]\n",
            " [2.31411864e-04 9.77905290e-01 2.18632984e-02]\n",
            " [2.00954947e-04 9.57416112e-01 4.23829326e-02]\n",
            " [9.54139433e-03 9.90441187e-01 1.74186947e-05]\n",
            " [7.09186639e-04 9.95903291e-01 3.38752261e-03]\n",
            " [3.86630035e-04 9.90592530e-01 9.02083948e-03]\n",
            " [1.47249971e-03 9.98139588e-01 3.87912061e-04]\n",
            " [4.74990307e-04 9.91379569e-01 8.14544026e-03]\n",
            " [1.76900233e-03 9.97897380e-01 3.33617314e-04]\n",
            " [1.48923918e-04 9.49868948e-01 4.99821280e-02]\n",
            " [6.42886187e-03 9.93505789e-01 6.53493482e-05]\n",
            " [1.65178275e-03 9.97433701e-01 9.14515914e-04]\n",
            " [1.14865321e-04 9.19653640e-01 8.02314944e-02]\n",
            " [4.57552284e-03 9.95340056e-01 8.44212540e-05]\n",
            " [1.78828968e-05 5.88824704e-01 4.11157413e-01]\n",
            " [2.07681689e-03 9.97595609e-01 3.27573844e-04]\n",
            " [1.05978359e-05 3.85954786e-01 6.14034617e-01]\n",
            " [2.17626749e-03 9.97394581e-01 4.29151169e-04]\n",
            " [1.02732831e-05 4.36103607e-01 5.63886120e-01]\n",
            " [3.50086276e-04 9.89403537e-01 1.02463763e-02]\n",
            " [1.53797734e-03 9.97607548e-01 8.54474455e-04]\n",
            " [1.06509218e-03 9.97064014e-01 1.87089423e-03]\n",
            " [1.95826040e-04 9.68675673e-01 3.11285007e-02]\n",
            " [1.95579228e-05 5.37926176e-01 4.62054266e-01]\n",
            " [1.44024938e-04 9.44633547e-01 5.52224280e-02]\n",
            " [2.51777198e-02 9.74818877e-01 3.40327794e-06]\n",
            " [1.89449742e-03 9.97746071e-01 3.59431895e-04]\n",
            " [4.98248570e-03 9.94959633e-01 5.78816453e-05]\n",
            " [2.78064842e-03 9.96979516e-01 2.39835663e-04]\n",
            " [1.54100761e-06 1.43872264e-01 8.56126195e-01]\n",
            " [8.72332870e-05 8.83747178e-01 1.16165589e-01]\n",
            " [3.64776898e-04 9.82673812e-01 1.69614112e-02]\n",
            " [3.30818078e-04 9.83545668e-01 1.61235144e-02]\n",
            " [1.58858896e-04 9.68536413e-01 3.13047285e-02]\n",
            " [1.47433977e-03 9.97551170e-01 9.74490423e-04]\n",
            " [3.94841654e-04 9.92353753e-01 7.25140537e-03]\n",
            " [2.45602313e-04 9.83228381e-01 1.65260162e-02]\n",
            " [2.82076013e-04 9.81010111e-01 1.87078130e-02]\n",
            " [1.42603028e-03 9.97830549e-01 7.43420394e-04]\n",
            " [7.90623494e-03 9.92070954e-01 2.28112550e-05]\n",
            " [4.15615844e-04 9.92053001e-01 7.53138287e-03]\n",
            " [2.15282910e-03 9.97383527e-01 4.63643825e-04]\n",
            " [8.69381522e-04 9.96796726e-01 2.33389271e-03]\n",
            " [1.18225576e-03 9.97455953e-01 1.36179099e-03]\n",
            " [2.75273720e-02 9.72469769e-01 2.85895899e-06]\n",
            " [9.04338422e-04 9.97053566e-01 2.04209606e-03]\n",
            " [2.34159010e-10 4.98646216e-04 9.99501354e-01]\n",
            " [4.52989451e-08 1.54604324e-02 9.84539522e-01]\n",
            " [6.23295894e-09 4.11048245e-03 9.95889511e-01]\n",
            " [5.49223367e-08 1.70734294e-02 9.82926516e-01]\n",
            " [1.66665711e-09 1.78844770e-03 9.98211551e-01]\n",
            " [8.12155981e-10 1.13396699e-03 9.98866032e-01]\n",
            " [5.79705244e-07 8.17421109e-02 9.18257309e-01]\n",
            " [1.15360792e-08 6.30236486e-03 9.93697624e-01]\n",
            " [8.50373112e-09 5.50430101e-03 9.94495690e-01]\n",
            " [1.30837528e-09 1.40113608e-03 9.98598863e-01]\n",
            " [8.32959752e-07 8.63753197e-02 9.13623847e-01]\n",
            " [5.33830224e-08 1.70297064e-02 9.82970240e-01]\n",
            " [2.41233010e-08 9.65550358e-03 9.90344472e-01]\n",
            " [1.10881910e-08 6.46961534e-03 9.93530374e-01]\n",
            " [1.17518776e-09 1.45608981e-03 9.98543909e-01]\n",
            " [1.18356562e-08 5.91894063e-03 9.94081048e-01]\n",
            " [2.35455870e-07 4.18727306e-02 9.58127034e-01]\n",
            " [4.51585780e-09 3.03206714e-03 9.96967928e-01]\n",
            " [5.08497752e-11 2.02926738e-04 9.99797073e-01]\n",
            " [6.86764611e-07 9.66782477e-02 9.03321066e-01]\n",
            " [4.59109217e-09 3.25977564e-03 9.96740220e-01]\n",
            " [5.26368548e-08 1.65879587e-02 9.83411989e-01]\n",
            " [6.89764744e-10 1.05005639e-03 9.98949943e-01]\n",
            " [1.21042075e-06 1.20508541e-01 8.79490249e-01]\n",
            " [2.55152476e-08 9.63295885e-03 9.90367016e-01]\n",
            " [1.66589417e-07 3.25970077e-02 9.67402826e-01]\n",
            " [2.91601649e-06 2.00376509e-01 7.99620575e-01]\n",
            " [3.46080706e-06 2.14731944e-01 7.85264595e-01]\n",
            " [3.40554444e-09 2.89800725e-03 9.97101989e-01]\n",
            " [1.69550033e-06 1.43833643e-01 8.56164661e-01]\n",
            " [1.03573743e-08 5.89498621e-03 9.94105003e-01]\n",
            " [1.90506286e-07 3.18244950e-02 9.68175314e-01]\n",
            " [1.63992228e-09 1.81280885e-03 9.98187190e-01]\n",
            " [1.35247836e-05 4.78484947e-01 5.21501528e-01]\n",
            " [4.99705699e-07 7.53138464e-02 9.24685654e-01]\n",
            " [1.60607234e-09 1.71286299e-03 9.98287135e-01]\n",
            " [2.49089233e-09 2.16639220e-03 9.97833605e-01]\n",
            " [3.05429867e-07 4.85713023e-02 9.51428392e-01]\n",
            " [5.00431361e-06 2.65030709e-01 7.34964287e-01]\n",
            " [7.80171155e-08 1.98741484e-02 9.80125774e-01]\n",
            " [1.68995443e-09 1.75191016e-03 9.98248088e-01]\n",
            " [5.77954997e-08 1.60933219e-02 9.83906620e-01]\n",
            " [4.52989451e-08 1.54604324e-02 9.84539522e-01]\n",
            " [1.69261801e-09 1.74735967e-03 9.98252639e-01]\n",
            " [1.09786323e-09 1.29987845e-03 9.98700120e-01]\n",
            " [1.50597953e-08 7.03975531e-03 9.92960230e-01]\n",
            " [9.01533184e-08 2.44274192e-02 9.75572491e-01]\n",
            " [1.70465326e-07 3.33637110e-02 9.66636119e-01]\n",
            " [1.12819463e-08 5.61290063e-03 9.94387088e-01]\n",
            " [6.40241002e-07 7.82524544e-02 9.21746905e-01]]\n",
            "\n",
            "Score\n",
            "0.98\n"
          ]
        }
      ],
      "source": [
        "# IRIS MLP\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier # neural network\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "x, y = load_iris(return_X_y=True, as_frame=True) # First param to denote \"dictionary form\", second param to declare as pandas df\n",
        "mlpLearner = MLPClassifier(max_iter = 1000, learning_rate_init = 0.01, batch_size=4, tol=0.001, verbose=False, activation='logistic') # change arguments to customize\n",
        "\n",
        "# Full Data\n",
        "mlpLearner.fit(x,y)\n",
        "predFull = mlpLearner.predict(x)\n",
        "probFull = mlpLearner.predict_proba(x)\n",
        "scoreFull = mlpLearner.score(x,y)\n",
        "\n",
        "print(\"Class Prediction\")\n",
        "print(predFull)\n",
        "print(\"\\nProbability Prediction\")\n",
        "print(probFull)\n",
        "print(\"\\nScore\")\n",
        "print(scoreFull)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone C"
      ],
      "metadata": {
        "id": "USJSI9lcFt_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Confusion Matrix Implementation"
      ],
      "metadata": {
        "id": "nlAgomguyFB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfusionMatrix:\n",
        "    def __init__(self, pred, val): # input has to be 2d array [numpy]\n",
        "        if (len(pred) != len(val)): raise ValueError('pred and val length is not the same!')\n",
        "        if (len(pred[0]) != len(val[0])): raise ValueError('number of class mismatch!')\n",
        "        self.pred = pred\n",
        "        self.val = val\n",
        "        self.count = len(val) # all data count\n",
        "        self.num_classes = len(val[0]) # number of classes\n",
        "        self.num_per_class = [0] * self.num_classes # data count per class\n",
        "\n",
        "        # col: predicted values, row: actual values [Confusion Matrix]\n",
        "        self.matrix = self.generate_matrix(pred, val)\n",
        "\n",
        "        self.true_positives = np.diag(self.matrix, k = 0) # True positives per class\n",
        "        self.false_positives = self.calculate_false_positives() # False positives per class\n",
        "        self.false_negatives = self.calculate_false_negatives() # False negatives per class\n",
        "        self.true_negatives = self.calculate_true_negatives() # True negatives per class\n",
        "\n",
        "        # Statistics per class\n",
        "        self.accuracies = np.nan_to_num(self.true_positives / self.num_per_class, copy = True, nan = 1.0) # Accuracy per class\n",
        "        self.precisions = self.calculate_precisions() # Precision per class\n",
        "        self.recalls = self.calculate_recalls() # Recall per class\n",
        "        self.f1_scores = self.calculate_f1_scores() # F1_Score per class\n",
        "\n",
        "        # Statistics for all\n",
        "        self.accuracy = np.sum(self.true_positives) / self.count # Accuracy for all\n",
        "        self.precision = np.average(self.precisions) # Precision for all\n",
        "        self.recall = np.average(self.recalls) # Recall for all\n",
        "        self.f1_score = np.average(self.f1_scores) # F1_Score for all\n",
        "\n",
        "    def generate_matrix(self, pred, val):\n",
        "        if (len(pred) != len(val)): return None\n",
        "        matrix = np.zeros(shape=(len(val[0]), len(val[0])))\n",
        "        for i in range(len(val)):\n",
        "            matrix[np.argmax(val[i])][np.argmax(pred[i])] += 1\n",
        "            self.num_per_class[np.argmax(val[i])] += 1\n",
        "        return matrix\n",
        "    \n",
        "    def calculate_false_positives(self):\n",
        "        arr = []\n",
        "        for i in range(self.num_classes):\n",
        "            col = self.matrix[:,i]\n",
        "            arr.append(np.sum(col) - col[i])\n",
        "        return np.array(arr)\n",
        "\n",
        "    def calculate_false_negatives(self):\n",
        "        arr = []\n",
        "        for i in range(self.num_classes):\n",
        "            row = self.matrix[i,:]\n",
        "            arr.append(np.sum(row) - row[i])\n",
        "        return np.array(arr)\n",
        "\n",
        "    def calculate_true_negatives(self):\n",
        "        arr = []\n",
        "        for i in range(self.num_classes):\n",
        "            mat = np.delete(np.delete(self.matrix, i, axis = 1), i, axis = 0)\n",
        "            arr.append(np.sum(mat))\n",
        "        return np.array(arr)\n",
        "\n",
        "    def calculate_precisions(self):\n",
        "        # true positive / (true positive + false positive)\n",
        "        return np.nan_to_num(self.true_positives / (self.true_positives + self.false_positives), copy = True, nan = 1.0)\n",
        "\n",
        "    def calculate_recalls(self):\n",
        "        # true positive / (true positive + false negative)\n",
        "        return np.nan_to_num(self.true_positives / (self.true_positives + self.false_negatives), copy = True, nan = 1.0)\n",
        "\n",
        "    def calculate_f1_scores(self):\n",
        "        # 2 * (precision * recall) / (precision + recall)\n",
        "        return np.nan_to_num(2 * (self.precisions * self.recalls) / (self.precisions + self.recalls), copy = True, nan = 1.0)"
      ],
      "metadata": {
        "id": "UeynKU03yCBX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Hasil Confusion Matrix dan Kinerja Implementasi FFNN"
      ],
      "metadata": {
        "id": "V_iOf409Zy-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Implementasi Confusion Matrix Kelompok\n",
        "predictTable = []\n",
        "for data in x_train:\n",
        "  # predictTable.append(np.argmax(ffnn_model.predict(data)))\n",
        "  predictStatus = [0 for _ in range(3)]\n",
        "  predictStatus[np.argmax(ffnn_model.predict(data))] = 1\n",
        "  predictTable.append(predictStatus)\n",
        "\n",
        "targetTable = []\n",
        "for data in y_train:\n",
        "  # targetTable.append(np.argmax(data))\n",
        "  targetTable.append(data)\n",
        "\n",
        "implPredict = np.array(predictTable)\n",
        "implTarget = np.array(targetTable)\n",
        "implCM = ConfusionMatrix(implPredict,implTarget)\n",
        "print(implCM.matrix)\n",
        "print(implCM.accuracy)\n",
        "print(implCM.precision)\n",
        "print(implCM.recall)\n",
        "print(implCM.f1_score)\n",
        "\n",
        "# Implementasi Confusion Matrix dan Scoring SKlearn\n",
        "skPredict = []\n",
        "skTarget = []\n",
        "for i in range(len(implPredict)):\n",
        "  skPredict.append(np.argmax(implPredict[i]))\n",
        "  skTarget.append(np.argmax(implTarget[i]))\n",
        "\n",
        "print(confusion_matrix(skTarget, skPredict))\n",
        "print(accuracy_score(skTarget, skPredict))\n",
        "print(precision_score(skTarget, skPredict, average='macro'))\n",
        "print(recall_score(skTarget, skPredict, average='macro'))\n",
        "print(f1_score(skTarget, skPredict, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y01yTzjhZW3V",
        "outputId": "df91319a-c177-4a55-ee78-328799b2bb6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[50.  0.  0.]\n",
            " [ 0. 46.  4.]\n",
            " [ 0.  0. 50.]]\n",
            "0.9733333333333334\n",
            "0.9753086419753086\n",
            "0.9733333333333333\n",
            "0.9732905982905983\n",
            "[[50  0  0]\n",
            " [ 0 46  4]\n",
            " [ 0  0 50]]\n",
            "0.9733333333333334\n",
            "0.9753086419753086\n",
            "0.9733333333333333\n",
            "0.9732905982905983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Hasil Confusion Matrix dan Kinerja MLP SKLearn\n"
      ],
      "metadata": {
        "id": "KiOokktFgcjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "x, y = load_iris(return_X_y=True, as_frame=True) \n",
        "mlpLearner = MLPClassifier(max_iter = 1000, learning_rate_init = 0.01, batch_size=4, tol=0.001, verbose=False, activation='logistic') \n",
        "mlpLearner.fit(x,y)\n",
        "predFull = mlpLearner.predict(x)\n",
        "probFull = mlpLearner.predict_proba(x)\n",
        "\n",
        "# Implementasi Confusion Matrix Kelompok\n",
        "predictTable = []\n",
        "for i in range(len(probFull)):\n",
        "  predictStatus = [0 for _ in range(len(probFull[i]))]\n",
        "  predictStatus[np.argmax(probFull[i])] = 1\n",
        "  predictTable.append(predictStatus)\n",
        "\n",
        "targetTable = []\n",
        "for data in y_train:\n",
        "  # targetTable.append(np.argmax(data))\n",
        "  targetTable.append(data)\n",
        "\n",
        "mlpPredict = np.array(predictTable)\n",
        "mlpTarget = np.array(targetTable)\n",
        "\n",
        "\n",
        "mlpCM = ConfusionMatrix(mlpPredict,mlpTarget)\n",
        "print(mlpCM.matrix)\n",
        "print(mlpCM.accuracy)\n",
        "print(mlpCM.precision)\n",
        "print(mlpCM.recall)\n",
        "print(mlpCM.f1_score)\n",
        "\n",
        "# Implementasi Confusion Matrix dan Scoring SKlearn\n",
        "print(confusion_matrix(y, predFull))\n",
        "print(accuracy_score(y, predFull))\n",
        "print(precision_score(y, predFull, average='macro'))\n",
        "print(recall_score(y, predFull, average='macro'))\n",
        "print(f1_score(y, predFull, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SuJ1c7DgjMC",
        "outputId": "e35506e4-5453-4c91-895b-726238b9d145"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[50.  0.  0.]\n",
            " [ 0. 45.  5.]\n",
            " [ 0.  0. 50.]]\n",
            "0.9666666666666667\n",
            "0.9696969696969697\n",
            "0.9666666666666667\n",
            "0.9665831244778613\n",
            "[[50  0  0]\n",
            " [ 0 45  5]\n",
            " [ 0  0 50]]\n",
            "0.9666666666666667\n",
            "0.9696969696969697\n",
            "0.9666666666666667\n",
            "0.9665831244778613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Hasil Confusion Matrix dan Kinerja Implementasi FFNN Split Data"
      ],
      "metadata": {
        "id": "Ev6XYdKovHH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ffnn_model_split = FFNN.generate_model(\n",
        "    4, [6, 4, 5, 3], ['sigmoid', 'relu', 'linear', 'softmax']\n",
        ")\n",
        "\n",
        "implx_train, x_test, imply_train, y_test = train_test_split(x_train, y_train, test_size=0.1,random_state=69)\n",
        "ffnn_model_split.fit(implx_train, imply_train, learning_rate = 0.01, batch_size = 4, err_threshold = 0.01, max_iter = 1000)\n",
        "\n",
        "# Implementasi Confusion Matrix Kelompok\n",
        "predictTable = []\n",
        "for data in x_test:\n",
        "  predictStatus = [0 for _ in range(3)]\n",
        "  predictStatus[np.argmax(ffnn_model_split.predict(data))] = 1\n",
        "  predictTable.append(predictStatus)\n",
        "\n",
        "targetTable = []\n",
        "for data in y_test:\n",
        "  targetTable.append(data)\n",
        "\n",
        "implPredict = np.array(predictTable)\n",
        "implTarget = np.array(targetTable)\n",
        "implCM = ConfusionMatrix(implPredict,implTarget)\n",
        "print(implCM.matrix)\n",
        "print(implCM.accuracy)\n",
        "print(implCM.precision)\n",
        "print(implCM.recall)\n",
        "print(implCM.f1_score)\n",
        "\n",
        "# Implementasi Confusion Matrix dan Scoring SKlearn\n",
        "skPredict = []\n",
        "skTarget = []\n",
        "for i in range(len(implPredict)):\n",
        "  skPredict.append(np.argmax(implPredict[i]))\n",
        "  skTarget.append(np.argmax(implTarget[i]))\n",
        "\n",
        "print(confusion_matrix(skTarget, skPredict))\n",
        "print(accuracy_score(skTarget, skPredict))\n",
        "print(precision_score(skTarget, skPredict, average='macro'))\n",
        "print(recall_score(skTarget, skPredict, average='macro'))\n",
        "print(f1_score(skTarget, skPredict, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a9b310-a68b-403d-9e51-38997e58df30",
        "id": "TvuvAq20vLHJ"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.144917\n",
            "Epoch 250, Loss: 0.085278\n",
            "Epoch 500, Loss: 0.087789\n",
            "Epoch 750, Loss: 0.111921\n",
            "[[6. 0. 0.]\n",
            " [0. 3. 0.]\n",
            " [0. 0. 6.]]\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "[[6 0 0]\n",
            " [0 3 0]\n",
            " [0 0 6]]\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Hasil Confusion Matrix dan Kinerja MLP SKLearn Split Data\n"
      ],
      "metadata": {
        "id": "S6Sl5laEjkpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "x, y = load_iris(return_X_y=True, as_frame=True) \n",
        "mlpLearner = MLPClassifier(max_iter = 1000, learning_rate_init = 0.01, batch_size=4, tol=0.001, verbose=False, activation='logistic') \n",
        "mlpx_train, mlpx_test, mlpy_train, mlpy_test = train_test_split(x , y, test_size=0.1,random_state=69)\n",
        "mlpLearner.fit(mlpx_train,mlpy_train)\n",
        "predSplit = mlpLearner.predict(mlpx_test)\n",
        "probSplit = mlpLearner.predict_proba(mlpx_test)\n",
        "\n",
        "# Implementasi Confusion Matrix Kelompok\n",
        "predictTable = []\n",
        "for i in range(len(probSplit)):\n",
        "  predictStatus = [0 for _ in range(len(probSplit[i]))]\n",
        "  predictStatus[np.argmax(probSplit[i])] = 1\n",
        "  predictTable.append(predictStatus)\n",
        "\n",
        "targetTable = []\n",
        "for data in mlpy_test:\n",
        "  # targetTable.append(np.argmax(data))\n",
        "  if (data == 0):\n",
        "    targetTable.append([1, 0, 0])\n",
        "  elif (data == 1):\n",
        "    targetTable.append([0, 1, 0])\n",
        "  elif (data == 2):\n",
        "    targetTable.append([0, 0, 1])\n",
        "  \n",
        "\n",
        "mlpPredict = np.array(predictTable)\n",
        "mlpTarget = np.array(targetTable)\n",
        "\n",
        "mlpCM = ConfusionMatrix(mlpPredict,mlpTarget)\n",
        "print(mlpCM.matrix)\n",
        "print(mlpCM.accuracy)\n",
        "print(mlpCM.precision)\n",
        "print(mlpCM.recall)\n",
        "print(mlpCM.f1_score)\n",
        "\n",
        "# Implementasi Confusion Matrix dan Scoring SKlearn\n",
        "print(confusion_matrix(mlpy_test, predSplit))\n",
        "print(accuracy_score(mlpy_test, predSplit))\n",
        "print(precision_score(mlpy_test, predSplit, average='macro'))\n",
        "print(recall_score(mlpy_test, predSplit, average='macro'))\n",
        "print(f1_score(mlpy_test, predSplit, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b97XZg1cjvwV",
        "outputId": "72e82048-448b-4925-ba61-2ca5b46f99b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6. 0. 0.]\n",
            " [0. 3. 0.]\n",
            " [0. 0. 6.]]\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "[[6 0 0]\n",
            " [0 3 0]\n",
            " [0 0 6]]\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 10 Fold Cross Validation "
      ],
      "metadata": {
        "id": "PKSCBrSdAOmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=10)\n",
        "kf.get_n_splits(x_train)\n",
        "sum_accuracy = 0\n",
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "sum_f1_score = 0\n",
        "\n",
        "ffnn_model_kfold = FFNN.generate_model(\n",
        "    4, [6, 4, 5, 3], ['sigmoid', 'relu', 'linear', 'softmax']\n",
        ")\n",
        "\n",
        "for train_index, test_index in kf.split(x_train):\n",
        "    x_fold_train, x_fold_test = x_train[train_index], x_train[test_index]\n",
        "    y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    ffnn_train = copy.deepcopy(ffnn_model_kfold)\n",
        "    ffnn_train.fit(x_fold_train, y_fold_train, learning_rate = 0.01, batch_size = 4, err_threshold = 0.01, max_iter = 1000)\n",
        "    prediction = []\n",
        "    target = []\n",
        "    \n",
        "    batch_predict = ffnn_train.predict(x_fold_test)\n",
        "\n",
        "    for predict in batch_predict:\n",
        "      temp = [0 for _ in range(len(predict))]\n",
        "      temp[np.argmax(predict)] = 1\n",
        "      prediction.append(temp)\n",
        "\n",
        "    cm = ConfusionMatrix(np.array(prediction), np.array(y_fold_test))\n",
        "    sum_accuracy += cm.accuracy\n",
        "    sum_precision += cm.precision\n",
        "    sum_recall += cm.recall\n",
        "    sum_f1_score += cm.f1_score\n",
        "\n",
        "print(\"Kinerja model (accuracy): \" + str(sum_accuracy/10))\n",
        "print(\"Kinerja model (precision): \" + str(sum_precision/10))\n",
        "print(\"Kinerja model (recall): \" + str(sum_recall/10))\n",
        "print(\"Kinerja model (F1 score): \" + str(sum_f1_score/10))"
      ],
      "metadata": {
        "id": "SGA_1_W9AaTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0bdc79-0589-4b4e-d95c-0613be822c45"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.100823\n",
            "Epoch 250, Loss: 0.792186\n",
            "Epoch 500, Loss: 0.051264\n",
            "Epoch 750, Loss: 0.053507\n",
            "Epoch 1, Loss: 1.100923\n",
            "Epoch 250, Loss: 0.849259\n",
            "Epoch 500, Loss: 0.051266\n",
            "Epoch 750, Loss: 0.053495\n",
            "Epoch 1, Loss: 1.100944\n",
            "Epoch 250, Loss: 0.737263\n",
            "Epoch 500, Loss: 0.050766\n",
            "Epoch 750, Loss: 0.053162\n",
            "Epoch 1, Loss: 1.159035\n",
            "Epoch 250, Loss: 0.337077\n",
            "Epoch 500, Loss: 0.049650\n",
            "Epoch 750, Loss: 0.051080\n",
            "Epoch 1, Loss: 1.176435\n",
            "Epoch 250, Loss: 0.374435\n",
            "Epoch 500, Loss: 0.046729\n",
            "Epoch 750, Loss: 0.049605\n",
            "Epoch 1, Loss: 1.176417\n",
            "Epoch 250, Loss: 0.329547\n",
            "Epoch 500, Loss: 0.039540\n",
            "Epoch 750, Loss: 0.034930\n",
            "Epoch 1, Loss: 1.185059\n",
            "Epoch 250, Loss: 0.291551\n",
            "Epoch 500, Loss: 0.048329\n",
            "Epoch 750, Loss: 0.051185\n",
            "Epoch 1, Loss: 1.187288\n",
            "Epoch 250, Loss: 0.268278\n",
            "Epoch 500, Loss: 0.056926\n",
            "Epoch 750, Loss: 0.055540\n",
            "Epoch 1, Loss: 1.187375\n",
            "Epoch 250, Loss: 0.277662\n",
            "Epoch 500, Loss: 0.027653\n",
            "Epoch 750, Loss: 0.025142\n",
            "Epoch 1, Loss: 1.187288\n",
            "Epoch 250, Loss: 0.261785\n",
            "Epoch 500, Loss: 0.046639\n",
            "Epoch 750, Loss: 0.050247\n",
            "Kinerja model (accuracy): 0.96\n",
            "Kinerja model (precision): 0.9\n",
            "Kinerja model (recall): 0.9866666666666667\n",
            "Kinerja model (F1 score): 0.8927659186279875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Save Implementation"
      ],
      "metadata": {
        "id": "S1tJIiTv-7vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_model.save()"
      ],
      "metadata": {
        "id": "pXC9N1qD_Avw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Load Implementation / 7. Prediksi Instance Baru"
      ],
      "metadata": {
        "id": "MRUjC800PIHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = _input(\"model.txt\", False)\n",
        "\n",
        "print(loaded_model.predict(x_train[1:4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZSXNDcHPNS7",
        "outputId": "953f6bd8-5dc7-4a44-a9fe-7ed78012239a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([9.99846294e-01, 1.53705577e-04, 3.71810746e-18]), array([9.99959616e-01, 4.03844199e-05, 7.00055221e-19]), array([9.98890520e-01, 1.10948013e-03, 4.39385443e-17])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Analisis Hasil Bagian 2 dan 3\n",
        "\n",
        "Pada bagian 2, kami melakukan penggunaan implementasi kelompok terkait confusion matrix dan penilaian kinerja pada 2 model yang berbeda. Model yang dilakukan penilaian adalah model yang kami bentuk menggunakan implementasi Milestone A dan Milestone B serta implementasi mlp library sklearn. Selain penggunaan implementasi kelompok, juga dilakukan penggunaan confusion matrix dan penilaian kinerja menggunakan library sklearn untuk perbandingan. Seperti yang dapat dilihat pada bagian 2, penggunaan confusion matrix dan penilaian kinerja implementasi kelompok serta penggunaan confusion matrix dan penilaian kinerja sklearn membuahkan hasil yang sama.\n",
        "\n",
        "Pada bagian 3, hal yang dilakukan sama seperti bagian 2. Namun demikian, terdapat sedikit perbedaan spek terkait data yang digunakan untuk membentuk model dan data yang digunakan untuk memprediksi menggunakan hasil pembelajaran model. Menggunakan split data dengan rasio 90:10, 90% data digunakan untuk melakukan fitting pada model dan 10% data digunakan untuk melakukan prediksi menggunakan model yang telah dilakukan pembelajaran sebelumnya. Sama seperti bagian 2, dapat dilihat pada bagian 3 bahwa penggunaan confusion matrix dan penilaian kinerja implementasi kelompok serta penggunaan confusion matrix dan penilaian kinerja sklearn membuahkan hasil yang sama.\n",
        "\n",
        "Dari hasil 2 bagian tersebut, dapat diperhatikan bahwa implementasi confusion matrix dan penilaian kinerja yang telah kelompok kami buat membuahkan hasil yang sama dengan menggunakan library sklearn. Maka dari itu, kesimpulan yang kami ambil terkait hal ini adalah bahwa implementasi confusion matrix dan penilaian kinerja yang dibuat oleh kelompok kami sudah benar. \n",
        "\n",
        "👍\n",
        "\n"
      ],
      "metadata": {
        "id": "kHm73O5_ylbL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TubesC_13519103.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}